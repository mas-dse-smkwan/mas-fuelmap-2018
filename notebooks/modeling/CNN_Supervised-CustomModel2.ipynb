{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Dense, Flatten,Merge,Conv2D, MaxPooling2D,BatchNormalization\n",
    "from keras.models import Sequential,Model\n",
    "#from sklearn.preprocessing import LabelEncoder,image\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedShuffleSplit,ShuffleSplit\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.applications.resnet50 import preprocess_input\n",
    "import numpy as np\n",
    "from IPython.display import display \n",
    "from PIL import Image\n",
    "import PIL.Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "import geopatches\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose the size of the tiles and extract as array using geopatches library\n",
    "dict_img = {'width': 6, 'height': 6, 'stride': 6, 'padding': 3}\n",
    "dict_label = {'width': 1, 'height': 1, 'stride': 1}\n",
    "X_raw, Y_raw = geopatches.create_data(['1154314_2014-07-23_RE2_3A_Analytic_clipped.tif'],dict_img,\\\n",
    "                                    ['ClusterRaster10_LandCover.tif'],dict_label)\n",
    "# Planetlabs needs to be divided by max uint16 = 65535 to be between 0 and 1.0\n",
    "X_raw = X_raw / 65535"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove classes that are too small\n",
    "list_remove = []\n",
    "for x in np.asarray(np.unique(Y_raw, return_counts=True)).T:\n",
    "    if x[1] < 400:\n",
    "        list_remove.append(x[0])\n",
    "idx = np.arange(Y_raw.shape[0])[np.in1d(Y_raw,list_remove)]\n",
    "X_raw = np.delete(X_raw,idx,0)\n",
    "Y_raw = np.delete(Y_raw,idx,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to create npy files for each tile\n",
    "def create_img_folder(X,Y_raw,train_index,test_index,val_index=None):\n",
    "    count=0\n",
    "    t=time.time()\n",
    "    %cd data_custom/train\n",
    "    for i in train_index:\n",
    "        count=count+1\n",
    "        np.save('RGB_'+str(i)+'.npy',X[i][:,:,:])\n",
    "        if count%10000==0:\n",
    "            print (time.time()-t)\n",
    "            print (count)\n",
    "            t=time.time()\n",
    "    if val_index:\n",
    "        %cd ..\n",
    "        %cd validate\n",
    "        for i in val_index:\n",
    "            np.save('RGB_'+str(i)+'.npy',X[i][:,:,:])\n",
    "    %cd ..\n",
    "    %cd test\n",
    "    count=0\n",
    "    for i in test_index:\n",
    "        np.save('RGB_'+str(i)+'.npy',X[i][:,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Split the images into train, test and validate\n",
    "sss = StratifiedShuffleSplit(n_splits=1, random_state=42, test_size=0.2)\n",
    "train_index, test_index = next(sss.split(X,Y_raw.flatten()))\n",
    "\n",
    "X_train = X[train_index]\n",
    "Y_train = Y_raw[train_index]\n",
    "\n",
    "#train_index, val_index = next(sss.split(X_train,Y_train.flatten()))\n",
    "\n",
    "X_test = X[test_index]\n",
    "Y_test = Y_raw[test_index]\n",
    "\n",
    "#X_val = X_train[val_index]\n",
    "#Y_val = Y_train[val_index]\n",
    "\n",
    "#X_train = X_train[train_index]\n",
    "#Y_train = Y_train[train_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123422"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dictionary to store train and test indexes\n",
    "partition = {'train':train_index,'test':test_index}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get labels\n",
    "labels = Y_raw.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ign:1 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64  InRelease\n",
      "Ign:2 http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1604/x86_64  InRelease\n",
      "Get:3 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64  Release [564 B]\n",
      "Get:4 http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1604/x86_64  Release [564 B]\n",
      "Get:5 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64  Release.gpg [801 B]\n",
      "Get:6 http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1604/x86_64  Release.gpg [801 B]\n",
      "Get:7 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64  Packages [98.4 kB]\n",
      "Get:8 http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1604/x86_64  Packages [20.8 kB]\n",
      "Get:9 http://archive.ubuntu.com/ubuntu xenial InRelease [247 kB]    \u001b[0m       \u001b[0m\u001b[33m\u001b[33m\n",
      "Get:10 http://ppa.launchpad.net/ubuntugis/ubuntugis-unstable/ubuntu xenial InRelease [23.8 kB]\n",
      "Get:11 http://security.ubuntu.com/ubuntu xenial-security InRelease [107 kB]    \u001b[0m\n",
      "Get:12 http://ppa.launchpad.net/ubuntugis/ubuntugis-unstable/ubuntu xenial/main amd64 Packages [64.0 kB]\n",
      "Get:13 http://security.ubuntu.com/ubuntu xenial-security/universe Sources [80.4 kB]\u001b[33m\u001b[33m\u001b[33m\n",
      "Get:14 http://archive.ubuntu.com/ubuntu xenial-updates InRelease [109 kB]      \u001b[0m\n",
      "Get:15 http://security.ubuntu.com/ubuntu xenial-security/main amd64 Packages [625 kB]33m\u001b[33m\n",
      "Get:16 http://archive.ubuntu.com/ubuntu xenial-backports InRelease [107 kB]\n",
      "Get:17 http://archive.ubuntu.com/ubuntu xenial/universe Sources [9802 kB]      \u001b[0m\u001b[33m\n",
      "Get:18 http://security.ubuntu.com/ubuntu xenial-security/restricted amd64 Packages [12.7 kB]\n",
      "Get:19 http://security.ubuntu.com/ubuntu xenial-security/universe amd64 Packages [439 kB]\n",
      "Get:20 http://security.ubuntu.com/ubuntu xenial-security/multiverse amd64 Packages [3759 B]3m\n",
      "Get:21 http://archive.ubuntu.com/ubuntu xenial/main amd64 Packages [1558 kB]   \u001b[0m\u001b[33m\u001b[33m\u001b[33m\n",
      "Get:22 http://archive.ubuntu.com/ubuntu xenial/restricted amd64 Packages [14.1 kB]\n",
      "Get:23 http://archive.ubuntu.com/ubuntu xenial/universe amd64 Packages [9827 kB]\n",
      "Get:24 http://archive.ubuntu.com/ubuntu xenial/multiverse amd64 Packages [176 kB]\n",
      "Get:25 http://archive.ubuntu.com/ubuntu xenial-updates/universe Sources [255 kB]\n",
      "Get:26 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 Packages [1002 kB]\n",
      "Get:27 http://archive.ubuntu.com/ubuntu xenial-updates/restricted amd64 Packages [13.1 kB]\n",
      "Get:28 http://archive.ubuntu.com/ubuntu xenial-updates/universe amd64 Packages [808 kB]\n",
      "Get:29 http://archive.ubuntu.com/ubuntu xenial-updates/multiverse amd64 Packages [18.8 kB]\n",
      "Get:30 http://archive.ubuntu.com/ubuntu xenial-backports/main amd64 Packages [5157 B]\n",
      "Get:31 http://archive.ubuntu.com/ubuntu xenial-backports/universe amd64 Packages [8075 B]\n",
      "Fetched 25.4 MB in 3s (6572 kB/s)[33m                       \u001b[0m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "2 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "libxext6 is already the newest version (2:1.3.3-1).\n",
      "libxext6 set to manually installed.\n",
      "libfontconfig1 is already the newest version (2.11.94-0ubuntu1.1).\n",
      "libfontconfig1 set to manually installed.\n",
      "The following NEW packages will be installed:\n",
      "  libice6 libsm6 libxrender1 x11-common\n",
      "0 upgraded, 4 newly installed, 0 to remove and 2 not upgraded.\n",
      "Need to get 95.9 kB of archives.\n",
      "After this operation, 597 kB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu xenial/main amd64 x11-common all 1:7.7+13ubuntu3 [22.4 kB]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu xenial/main amd64 libice6 amd64 2:1.0.9-1 [39.2 kB]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu xenial/main amd64 libsm6 amd64 2:1.2.2-1 [15.8 kB]\n",
      "Get:4 http://archive.ubuntu.com/ubuntu xenial/main amd64 libxrender1 amd64 1:0.9.9-0ubuntu1 [18.5 kB]\n",
      "Fetched 95.9 kB in 0s (114 kB/s)      \u001b[0m\u001b[33m\n",
      "debconf: delaying package configuration, since apt-utils is not installed\n",
      "\n",
      "\u001b7\u001b[0;23r\u001b8\u001b[1ASelecting previously unselected package x11-common.\n",
      "(Reading database ... 36878 files and directories currently installed.)\n",
      "Preparing to unpack .../x11-common_1%3a7.7+13ubuntu3_all.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  0%]\u001b[49m\u001b[39m [..........................................................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  4%]\u001b[49m\u001b[39m [###.......................................................] \u001b8Unpacking x11-common (1:7.7+13ubuntu3) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  9%]\u001b[49m\u001b[39m [######....................................................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 14%]\u001b[49m\u001b[39m [#########.................................................] \u001b8Selecting previously unselected package libice6:amd64.\n",
      "Preparing to unpack .../libice6_2%3a1.0.9-1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 19%]\u001b[49m\u001b[39m [############..............................................] \u001b8Unpacking libice6:amd64 (2:1.0.9-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 23%]\u001b[49m\u001b[39m [##############............................................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 28%]\u001b[49m\u001b[39m [#################.........................................] \u001b8Selecting previously unselected package libsm6:amd64.\n",
      "Preparing to unpack .../libsm6_2%3a1.2.2-1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 33%]\u001b[49m\u001b[39m [####################......................................] \u001b8Unpacking libsm6:amd64 (2:1.2.2-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 38%]\u001b[49m\u001b[39m [#######################...................................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 42%]\u001b[49m\u001b[39m [#########################.................................] \u001b8Selecting previously unselected package libxrender1:amd64.\n",
      "Preparing to unpack .../libxrender1_1%3a0.9.9-0ubuntu1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 47%]\u001b[49m\u001b[39m [############################..............................] \u001b8Unpacking libxrender1:amd64 (1:0.9.9-0ubuntu1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 52%]\u001b[49m\u001b[39m [###############################...........................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 57%]\u001b[49m\u001b[39m [##################################........................] \u001b8Processing triggers for systemd (229-4ubuntu21.2) ...\n",
      "Setting up x11-common (1:7.7+13ubuntu3) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 61%]\u001b[49m\u001b[39m [####################################......................] \u001b8debconf: unable to initialize frontend: Dialog\n",
      "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76.)\n",
      "debconf: falling back to frontend: Readline\n",
      "update-rc.d: warning: start and stop actions are no longer supported; falling back to defaults\n",
      "invoke-rc.d: could not determine current runlevel\n",
      "invoke-rc.d: policy-rc.d denied execution of start.\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 66%]\u001b[49m\u001b[39m [#######################################...................] \u001b8Setting up libice6:amd64 (2:1.0.9-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 71%]\u001b[49m\u001b[39m [##########################################................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 76%]\u001b[49m\u001b[39m [#############################################.............] \u001b8Setting up libsm6:amd64 (2:1.2.2-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 80%]\u001b[49m\u001b[39m [###############################################...........] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 85%]\u001b[49m\u001b[39m [##################################################........] \u001b8Setting up libxrender1:amd64 (1:0.9.9-0ubuntu1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 90%]\u001b[49m\u001b[39m [#####################################################.....] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 95%]\u001b[49m\u001b[39m [########################################################..] \u001b8Processing triggers for systemd (229-4ubuntu21.2) ...\n",
      "Processing triggers for libc-bin (2.23-0ubuntu10) ...\n",
      "\n",
      "\u001b7\u001b[0;24r\u001b8\u001b[1A\u001b[J"
     ]
    }
   ],
   "source": [
    "!apt update && apt install -y libsm6 libxext6 libfontconfig1 libxrender1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/notebooks\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/notebooks\n"
     ]
    }
   ],
   "source": [
    "%cd ../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/notebooks/data_custom/train\n",
      "2.058041572570801\n",
      "10000\n",
      "2.0400688648223877\n",
      "20000\n",
      "2.033390522003174\n",
      "30000\n",
      "2.1457667350769043\n",
      "40000\n",
      "2.1008222103118896\n",
      "50000\n",
      "2.094082832336426\n",
      "60000\n",
      "2.075732469558716\n",
      "70000\n",
      "2.0327062606811523\n",
      "80000\n",
      "2.043717384338379\n",
      "90000\n",
      "1.9963915348052979\n",
      "100000\n",
      "2.031646728515625\n",
      "110000\n",
      "2.0857222080230713\n",
      "120000\n",
      "/notebooks/data_custom\n",
      "/notebooks/data_custom/test\n"
     ]
    }
   ],
   "source": [
    "#Create images in respective folders\n",
    "create_img_folder(X,Y_raw,train_index,test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Assign each class a sequential value from 0 to n\n",
    "classes={}\n",
    "for i in range(len(set(labels))):\n",
    "    classes[i]=list(set(labels))[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace labels with new values\n",
    "for i in range(len(labels)):\n",
    "    labels[i]=(list(classes.keys())[list(classes.values()).index(labels[i])]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class to generate image batches to be fed to the model\n",
    "import numpy as np\n",
    "import keras\n",
    "\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, list_IDs, labels, batch_size=128, dim=(216,216), n_channels=3,\n",
    "                 n_classes=14, shuffle=True, sampletype='train'):\n",
    "        #print(1)\n",
    "        'Initialization'\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.sampletype = sampletype\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        #print(2)\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        #print(3)\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        #print(4)\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        #print(5)\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
    "        #X1 = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
    "        #print(X.shape)\n",
    "        y = np.empty((self.batch_size), dtype=int)\n",
    "        #print (list_IDs_temp)\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            # Store sample\n",
    "            X[i,] = np.load('data_custom/'+self.sampletype+'/RGB_' + str(ID) + '.npy')\n",
    "            #X1[i,] = np.load('data_custom/'+self.sampletype+'/NRR_' + str(ID) + '.npy')\n",
    "            #print (i)\n",
    "            #print ('ID:'+str(ID))\n",
    "            #print ('label:'+str(self.labels[ID]))\n",
    "            # Store class\n",
    "            y[i] = self.labels[ID]\n",
    "        #print (y[52])\n",
    "\n",
    "        return X, keras.utils.to_categorical(y, num_classes=self.n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "params_train = {'dim': (12,12),\n",
    "          'batch_size': 128,\n",
    "          'n_classes': 11,\n",
    "          'n_channels': 5,\n",
    "          'shuffle': True,\n",
    "          'sampletype':'train'}\n",
    "\n",
    "params_test = {'dim': (12,12),\n",
    "          'batch_size': 128,\n",
    "          'n_classes': 11,\n",
    "          'n_channels': 5,\n",
    "          'shuffle': True,\n",
    "          'sampletype':'test'}\n",
    "\n",
    "# Generators\n",
    "training_generator = DataGenerator(partition['train'], labels, **params_train)\n",
    "validation_generator = DataGenerator(partition['test'], labels, **params_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "model = Sequential()\n",
    "# input: 100x100 images with 3 channels -> (100, 100, 3) tensors.\n",
    "# this applies 32 convolution filters of size 3x3 each.\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(12, 12, 5)))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "\n",
    "#model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(len(set(labels)), activation='softmax'))\n",
    "\n",
    "#sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "#model.compile(loss='categorical_crossentropy', optimizer=sgd)\n",
    "\n",
    "#model.fit(x_train, y_train, batch_size=32, epochs=10)\n",
    "#score = model.evaluate(x_test, y_test, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 10, 10, 32)        1472      \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 8, 8, 32)          9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 4, 4, 32)          0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 4, 4, 32)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 4, 4, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 2, 2, 64)          18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 1, 1, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 1, 1, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               16640     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 14)                3598      \n",
      "=================================================================\n",
      "Total params: 49,838\n",
      "Trainable params: 49,646\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = optimizers.SGD(lr=0.0001, decay=1e-6, momentum=0.9, nesterov=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint,EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopper = EarlyStopping(min_delta=0.001, patience=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint\n",
    "filepath=\"weights_cnn_custom_bn_11.best.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint,early_stopper]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/notebooks\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/notebooks\n"
     ]
    }
   ],
   "source": [
    "%cd ../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, 12, 12, 5)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weight = class_weight.compute_class_weight('balanced',\n",
    "                                                 np.unique(Y_train.flatten()),\n",
    "                                                 Y_train.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.61289736,  0.41571626,  0.61938624,  0.70306296,  3.24846028,\n",
       "        9.12952141, 20.70144247,  0.25359782,  1.19135505, 11.72432792,\n",
       "       21.78676081])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "961/964 [============================>.] - ETA: 0s - loss: 1.7666 - acc: 0.4348\n",
      "964/964 [==============================] - 16s 16ms/step - loss: 1.7656 - acc: 0.4352 - val_loss: 1.4318 - val_acc: 0.5377\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.53770, saving model to weights_cnn_custom_bn_11.best.hdf5\n",
      "Epoch 2/500\n",
      "964/964 [==============================] - 14s 15ms/step - loss: 1.3819 - acc: 0.5366 - val_loss: 1.2685 - val_acc: 0.5796\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.53770 to 0.57955, saving model to weights_cnn_custom_bn_11.best.hdf5\n",
      "Epoch 3/500\n",
      "964/964 [==============================] - 14s 15ms/step - loss: 1.2797 - acc: 0.5559 - val_loss: 1.2036 - val_acc: 0.5847\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.57955 to 0.58467, saving model to weights_cnn_custom_bn_11.best.hdf5\n",
      "Epoch 4/500\n",
      "964/964 [==============================] - 14s 14ms/step - loss: 1.2267 - acc: 0.5680 - val_loss: 1.1545 - val_acc: 0.5937\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.58467 to 0.59365, saving model to weights_cnn_custom_bn_11.best.hdf5\n",
      "Epoch 5/500\n",
      "964/964 [==============================] - 14s 14ms/step - loss: 1.1985 - acc: 0.5749 - val_loss: 1.1516 - val_acc: 0.5979\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.59365 to 0.59790, saving model to weights_cnn_custom_bn_11.best.hdf5\n",
      "Epoch 6/500\n",
      "964/964 [==============================] - 14s 15ms/step - loss: 1.1788 - acc: 0.5784 - val_loss: 1.1304 - val_acc: 0.6072\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.59790 to 0.60724, saving model to weights_cnn_custom_bn_11.best.hdf5\n",
      "Epoch 7/500\n",
      "964/964 [==============================] - 14s 14ms/step - loss: 1.1677 - acc: 0.5821 - val_loss: 1.1352 - val_acc: 0.5926\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.60724\n",
      "Epoch 8/500\n",
      "964/964 [==============================] - 14s 15ms/step - loss: 1.1572 - acc: 0.5844 - val_loss: 1.1124 - val_acc: 0.6061\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.60724\n",
      "Epoch 9/500\n",
      "964/964 [==============================] - 14s 15ms/step - loss: 1.1512 - acc: 0.5840 - val_loss: 1.1355 - val_acc: 0.5872\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.60724\n",
      "Epoch 10/500\n",
      "964/964 [==============================] - 14s 14ms/step - loss: 1.1435 - acc: 0.5862 - val_loss: 1.1050 - val_acc: 0.6065\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.60724\n",
      "Epoch 11/500\n",
      "964/964 [==============================] - 14s 14ms/step - loss: 1.1379 - acc: 0.5880 - val_loss: 1.1020 - val_acc: 0.5996\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.60724\n",
      "Epoch 12/500\n",
      "964/964 [==============================] - 14s 15ms/step - loss: 1.1349 - acc: 0.5875 - val_loss: 1.0885 - val_acc: 0.6109\n",
      "\n",
      "Epoch 00012: val_acc improved from 0.60724 to 0.61090, saving model to weights_cnn_custom_bn_11.best.hdf5\n",
      "Epoch 13/500\n",
      "964/964 [==============================] - 14s 14ms/step - loss: 1.1303 - acc: 0.5890 - val_loss: 1.1351 - val_acc: 0.5789\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.61090\n",
      "Epoch 14/500\n",
      "964/964 [==============================] - 14s 15ms/step - loss: 1.1263 - acc: 0.5903 - val_loss: 1.1004 - val_acc: 0.5944\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.61090\n",
      "Epoch 15/500\n",
      "964/964 [==============================] - 14s 15ms/step - loss: 1.1217 - acc: 0.5928 - val_loss: 1.0719 - val_acc: 0.6092\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.61090\n",
      "Epoch 16/500\n",
      "964/964 [==============================] - 14s 14ms/step - loss: 1.1204 - acc: 0.5922 - val_loss: 1.0648 - val_acc: 0.6174\n",
      "\n",
      "Epoch 00016: val_acc improved from 0.61090 to 0.61741, saving model to weights_cnn_custom_bn_11.best.hdf5\n",
      "Epoch 17/500\n",
      "964/964 [==============================] - 14s 15ms/step - loss: 1.1150 - acc: 0.5931 - val_loss: 1.0674 - val_acc: 0.6131\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.61741\n",
      "Epoch 18/500\n",
      "964/964 [==============================] - 14s 15ms/step - loss: 1.1100 - acc: 0.5957 - val_loss: 1.0879 - val_acc: 0.6125\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.61741\n",
      "Epoch 19/500\n",
      "964/964 [==============================] - 14s 15ms/step - loss: 1.1084 - acc: 0.5951 - val_loss: 1.0692 - val_acc: 0.6091\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.61741\n",
      "Epoch 20/500\n",
      "964/964 [==============================] - 14s 15ms/step - loss: 1.1059 - acc: 0.5961 - val_loss: 1.0842 - val_acc: 0.6020\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.61741\n",
      "Epoch 21/500\n",
      "964/964 [==============================] - 14s 14ms/step - loss: 1.1043 - acc: 0.5958 - val_loss: 1.0552 - val_acc: 0.6179\n",
      "\n",
      "Epoch 00021: val_acc improved from 0.61741 to 0.61787, saving model to weights_cnn_custom_bn_11.best.hdf5\n",
      "Epoch 22/500\n",
      "964/964 [==============================] - 14s 15ms/step - loss: 1.1014 - acc: 0.5964 - val_loss: 1.0792 - val_acc: 0.6090\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.61787\n",
      "Epoch 23/500\n",
      "964/964 [==============================] - 14s 15ms/step - loss: 1.0982 - acc: 0.5987 - val_loss: 1.0600 - val_acc: 0.6154\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.61787\n",
      "Epoch 24/500\n",
      "964/964 [==============================] - 14s 15ms/step - loss: 1.0967 - acc: 0.5981 - val_loss: 1.0488 - val_acc: 0.6164\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.61787\n",
      "Epoch 25/500\n",
      "964/964 [==============================] - 14s 15ms/step - loss: 1.0944 - acc: 0.5984 - val_loss: 1.1039 - val_acc: 0.5989\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.61787\n",
      "Epoch 26/500\n",
      "964/964 [==============================] - 14s 15ms/step - loss: 1.0937 - acc: 0.5993 - val_loss: 1.0772 - val_acc: 0.6079\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.61787\n",
      "Epoch 27/500\n",
      "964/964 [==============================] - 14s 15ms/step - loss: 1.0913 - acc: 0.6010 - val_loss: 1.0637 - val_acc: 0.6134\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.61787\n",
      "Epoch 28/500\n",
      "964/964 [==============================] - 14s 14ms/step - loss: 1.0893 - acc: 0.6002 - val_loss: 1.0697 - val_acc: 0.6101\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.61787\n",
      "Epoch 29/500\n",
      "964/964 [==============================] - 14s 15ms/step - loss: 1.0898 - acc: 0.6003 - val_loss: 1.1466 - val_acc: 0.5793\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.61787\n",
      "Epoch 30/500\n",
      "964/964 [==============================] - 14s 15ms/step - loss: 1.0859 - acc: 0.6018 - val_loss: 1.0738 - val_acc: 0.6111\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.61787\n",
      "Epoch 31/500\n",
      "964/964 [==============================] - 14s 15ms/step - loss: 1.0837 - acc: 0.6025 - val_loss: 1.0630 - val_acc: 0.6130\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.61787\n",
      "Epoch 32/500\n",
      "964/964 [==============================] - 15s 16ms/step - loss: 1.0826 - acc: 0.6022 - val_loss: 1.0501 - val_acc: 0.6203\n",
      "\n",
      "Epoch 00032: val_acc improved from 0.61787 to 0.62027, saving model to weights_cnn_custom_bn_11.best.hdf5\n",
      "Epoch 33/500\n",
      "964/964 [==============================] - 14s 14ms/step - loss: 1.0795 - acc: 0.6032 - val_loss: 1.1138 - val_acc: 0.5899\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.62027\n",
      "Epoch 34/500\n",
      "964/964 [==============================] - 14s 15ms/step - loss: 1.0804 - acc: 0.6027 - val_loss: 1.0557 - val_acc: 0.6148\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.62027\n",
      "Epoch 35/500\n",
      "964/964 [==============================] - 14s 14ms/step - loss: 1.0794 - acc: 0.6030 - val_loss: 1.0548 - val_acc: 0.6150\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.62027\n",
      "Epoch 36/500\n",
      "964/964 [==============================] - 14s 15ms/step - loss: 1.0773 - acc: 0.6027 - val_loss: 1.0390 - val_acc: 0.6217\n",
      "\n",
      "Epoch 00036: val_acc improved from 0.62027 to 0.62169, saving model to weights_cnn_custom_bn_11.best.hdf5\n",
      "Epoch 37/500\n",
      "964/964 [==============================] - 15s 15ms/step - loss: 1.0749 - acc: 0.6050 - val_loss: 1.0398 - val_acc: 0.6163\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.62169\n",
      "Epoch 38/500\n",
      "964/964 [==============================] - 14s 15ms/step - loss: 1.0757 - acc: 0.6060 - val_loss: 1.0360 - val_acc: 0.6242\n",
      "\n",
      "Epoch 00038: val_acc improved from 0.62169 to 0.62422, saving model to weights_cnn_custom_bn_11.best.hdf5\n",
      "Epoch 39/500\n",
      "964/964 [==============================] - 14s 15ms/step - loss: 1.0731 - acc: 0.6057 - val_loss: 1.0948 - val_acc: 0.6055\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.62422\n",
      "Epoch 40/500\n",
      "964/964 [==============================] - 14s 15ms/step - loss: 1.0732 - acc: 0.6040 - val_loss: 1.0694 - val_acc: 0.6070\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.62422\n",
      "Epoch 41/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "964/964 [==============================] - 14s 15ms/step - loss: 1.0711 - acc: 0.6054 - val_loss: 1.0621 - val_acc: 0.6148\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.62422\n",
      "Epoch 42/500\n",
      "964/964 [==============================] - 14s 15ms/step - loss: 1.0714 - acc: 0.6044 - val_loss: 1.0587 - val_acc: 0.6115\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.62422\n",
      "Epoch 43/500\n",
      "964/964 [==============================] - 14s 15ms/step - loss: 1.0703 - acc: 0.6066 - val_loss: 1.0820 - val_acc: 0.6026\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.62422\n",
      "Epoch 44/500\n",
      "964/964 [==============================] - 14s 14ms/step - loss: 1.0660 - acc: 0.6080 - val_loss: 1.0504 - val_acc: 0.6082\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.62422\n",
      "Epoch 45/500\n",
      "964/964 [==============================] - 14s 15ms/step - loss: 1.0692 - acc: 0.6066 - val_loss: 1.0385 - val_acc: 0.6207\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.62422\n",
      "Epoch 46/500\n",
      "964/964 [==============================] - 13s 14ms/step - loss: 1.0671 - acc: 0.6071 - val_loss: 1.0555 - val_acc: 0.6060\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.62422\n",
      "Epoch 47/500\n",
      "964/964 [==============================] - 14s 14ms/step - loss: 1.0646 - acc: 0.6066 - val_loss: 1.0425 - val_acc: 0.6201\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.62422\n",
      "Epoch 48/500\n",
      "964/964 [==============================] - 14s 15ms/step - loss: 1.0637 - acc: 0.6079 - val_loss: 1.0276 - val_acc: 0.6253\n",
      "\n",
      "Epoch 00048: val_acc improved from 0.62422 to 0.62529, saving model to weights_cnn_custom_bn_11.best.hdf5\n",
      "Epoch 49/500\n",
      "964/964 [==============================] - 14s 15ms/step - loss: 1.0623 - acc: 0.6077 - val_loss: 1.0671 - val_acc: 0.6107\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.62529\n",
      "Epoch 50/500\n",
      "964/964 [==============================] - 14s 15ms/step - loss: 1.0627 - acc: 0.6074 - val_loss: 1.0318 - val_acc: 0.6247\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.62529\n",
      "Epoch 51/500\n",
      "964/964 [==============================] - 14s 15ms/step - loss: 1.0624 - acc: 0.6077 - val_loss: 1.0191 - val_acc: 0.6290\n",
      "\n",
      "Epoch 00051: val_acc improved from 0.62529 to 0.62902, saving model to weights_cnn_custom_bn_11.best.hdf5\n",
      "Epoch 52/500\n",
      "964/964 [==============================] - 14s 14ms/step - loss: 1.0614 - acc: 0.6091 - val_loss: 1.0436 - val_acc: 0.6213\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.62902\n",
      "Epoch 53/500\n",
      "964/964 [==============================] - 16s 16ms/step - loss: 1.0601 - acc: 0.6094 - val_loss: 1.0477 - val_acc: 0.6221\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.62902\n",
      "Epoch 54/500\n",
      "964/964 [==============================] - 18s 18ms/step - loss: 1.0595 - acc: 0.6085 - val_loss: 1.0333 - val_acc: 0.6261\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.62902\n",
      "Epoch 55/500\n",
      "964/964 [==============================] - 15s 15ms/step - loss: 1.0576 - acc: 0.6100 - val_loss: 1.0760 - val_acc: 0.6095\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.62902\n",
      "Epoch 56/500\n",
      "964/964 [==============================] - 14s 15ms/step - loss: 1.0570 - acc: 0.6097 - val_loss: 1.0507 - val_acc: 0.6159\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.62902\n",
      "Epoch 57/500\n",
      "964/964 [==============================] - 14s 14ms/step - loss: 1.0562 - acc: 0.6099 - val_loss: 1.0662 - val_acc: 0.6106\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.62902\n",
      "Epoch 58/500\n",
      "964/964 [==============================] - 14s 14ms/step - loss: 1.0566 - acc: 0.6095 - val_loss: 1.0342 - val_acc: 0.6236\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.62902\n",
      "Epoch 59/500\n",
      "964/964 [==============================] - 14s 14ms/step - loss: 1.0564 - acc: 0.6091 - val_loss: 1.0406 - val_acc: 0.6221\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.62902\n",
      "Epoch 60/500\n",
      "964/964 [==============================] - 14s 14ms/step - loss: 1.0539 - acc: 0.6115 - val_loss: 1.0375 - val_acc: 0.6180\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.62902\n",
      "Epoch 61/500\n",
      "964/964 [==============================] - 15s 15ms/step - loss: 1.0546 - acc: 0.6096 - val_loss: 1.0127 - val_acc: 0.6290\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.62902\n",
      "Epoch 62/500\n",
      "964/964 [==============================] - 14s 14ms/step - loss: 1.0535 - acc: 0.6102 - val_loss: 1.0332 - val_acc: 0.6179\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.62902\n",
      "Epoch 63/500\n",
      "964/964 [==============================] - 14s 14ms/step - loss: 1.0519 - acc: 0.6109 - val_loss: 1.0413 - val_acc: 0.6222\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.62902\n",
      "Epoch 64/500\n",
      "964/964 [==============================] - 14s 15ms/step - loss: 1.0526 - acc: 0.6104 - val_loss: 1.0434 - val_acc: 0.6145\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.62902\n",
      "Epoch 65/500\n",
      "964/964 [==============================] - 14s 15ms/step - loss: 1.0514 - acc: 0.6102 - val_loss: 1.0314 - val_acc: 0.6207\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.62902\n",
      "Epoch 66/500\n",
      "964/964 [==============================] - 14s 14ms/step - loss: 1.0502 - acc: 0.6119 - val_loss: 1.0548 - val_acc: 0.6135\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.62902\n",
      "Epoch 67/500\n",
      "964/964 [==============================] - 13s 14ms/step - loss: 1.0506 - acc: 0.6123 - val_loss: 1.0653 - val_acc: 0.6080\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.62902\n",
      "Epoch 68/500\n",
      "964/964 [==============================] - 14s 15ms/step - loss: 1.0490 - acc: 0.6112 - val_loss: 1.0758 - val_acc: 0.6043\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.62902\n",
      "Epoch 69/500\n",
      "964/964 [==============================] - 14s 14ms/step - loss: 1.0478 - acc: 0.6134 - val_loss: 1.0198 - val_acc: 0.6273\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.62902\n",
      "Epoch 70/500\n",
      "964/964 [==============================] - 14s 15ms/step - loss: 1.0493 - acc: 0.6114 - val_loss: 1.0406 - val_acc: 0.6183\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.62902\n",
      "Epoch 71/500\n",
      "964/964 [==============================] - 14s 14ms/step - loss: 1.0480 - acc: 0.6117 - val_loss: 1.0117 - val_acc: 0.6272\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.62902\n",
      "Epoch 72/500\n",
      "964/964 [==============================] - 14s 15ms/step - loss: 1.0459 - acc: 0.6140 - val_loss: 1.0488 - val_acc: 0.6107\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.62902\n",
      "Epoch 73/500\n",
      "964/964 [==============================] - 15s 15ms/step - loss: 1.0472 - acc: 0.6127 - val_loss: 1.0711 - val_acc: 0.6092\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.62902\n",
      "Epoch 74/500\n",
      "964/964 [==============================] - 14s 15ms/step - loss: 1.0459 - acc: 0.6126 - val_loss: 1.0351 - val_acc: 0.6230\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.62902\n",
      "Epoch 75/500\n",
      "964/964 [==============================] - 14s 15ms/step - loss: 1.0461 - acc: 0.6122 - val_loss: 1.0452 - val_acc: 0.6172\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.62902\n",
      "Epoch 76/500\n",
      "964/964 [==============================] - 14s 15ms/step - loss: 1.0449 - acc: 0.6130 - val_loss: 1.0230 - val_acc: 0.6283\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.62902\n",
      "Epoch 77/500\n",
      "964/964 [==============================] - 14s 15ms/step - loss: 1.0446 - acc: 0.6127 - val_loss: 1.0328 - val_acc: 0.6204\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.62902\n",
      "Epoch 78/500\n",
      "964/964 [==============================] - 14s 15ms/step - loss: 1.0450 - acc: 0.6131 - val_loss: 1.0403 - val_acc: 0.6209\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.62902\n",
      "Epoch 79/500\n",
      "964/964 [==============================] - 14s 15ms/step - loss: 1.0460 - acc: 0.6121 - val_loss: 1.0838 - val_acc: 0.6053\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.62902\n",
      "Epoch 80/500\n",
      "964/964 [==============================] - 15s 15ms/step - loss: 1.0432 - acc: 0.6136 - val_loss: 1.0188 - val_acc: 0.6263\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 0.62902\n",
      "Epoch 81/500\n",
      "964/964 [==============================] - 14s 14ms/step - loss: 1.0424 - acc: 0.6128 - val_loss: 1.0214 - val_acc: 0.6244\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 0.62902\n",
      "Epoch 82/500\n",
      "964/964 [==============================] - 14s 14ms/step - loss: 1.0416 - acc: 0.6144 - val_loss: 1.0157 - val_acc: 0.6259\n",
      "\n",
      "Epoch 00082: val_acc did not improve from 0.62902\n",
      "Epoch 83/500\n",
      "964/964 [==============================] - 14s 14ms/step - loss: 1.0426 - acc: 0.6123 - val_loss: 1.0369 - val_acc: 0.6249\n",
      "\n",
      "Epoch 00083: val_acc did not improve from 0.62902\n",
      "Epoch 84/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "964/964 [==============================] - 14s 15ms/step - loss: 1.0414 - acc: 0.6137 - val_loss: 1.0079 - val_acc: 0.6309\n",
      "\n",
      "Epoch 00084: val_acc improved from 0.62902 to 0.63093, saving model to weights_cnn_custom_bn_11.best.hdf5\n",
      "Epoch 85/500\n",
      "964/964 [==============================] - 14s 14ms/step - loss: 1.0399 - acc: 0.6148 - val_loss: 1.0259 - val_acc: 0.6238\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 0.63093\n",
      "Epoch 86/500\n",
      "964/964 [==============================] - 15s 15ms/step - loss: 1.0391 - acc: 0.6137 - val_loss: 1.0339 - val_acc: 0.6208\n",
      "\n",
      "Epoch 00086: val_acc did not improve from 0.63093\n",
      "Epoch 87/500\n",
      "964/964 [==============================] - 14s 14ms/step - loss: 1.0386 - acc: 0.6143 - val_loss: 1.0089 - val_acc: 0.6298\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.63093\n",
      "Epoch 88/500\n",
      "964/964 [==============================] - 14s 15ms/step - loss: 1.0403 - acc: 0.6134 - val_loss: 1.0352 - val_acc: 0.6192\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 0.63093\n",
      "Epoch 89/500\n",
      "964/964 [==============================] - 14s 15ms/step - loss: 1.0403 - acc: 0.6138 - val_loss: 1.0973 - val_acc: 0.5999\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 0.63093\n",
      "Epoch 90/500\n",
      "964/964 [==============================] - 14s 14ms/step - loss: 1.0374 - acc: 0.6150 - val_loss: 1.0339 - val_acc: 0.6189\n",
      "\n",
      "Epoch 00090: val_acc did not improve from 0.63093\n",
      "Epoch 91/500\n",
      "964/964 [==============================] - 14s 14ms/step - loss: 1.0395 - acc: 0.6147 - val_loss: 1.0508 - val_acc: 0.6121\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 0.63093\n",
      "Epoch 92/500\n",
      "964/964 [==============================] - 14s 15ms/step - loss: 1.0392 - acc: 0.6135 - val_loss: 1.0059 - val_acc: 0.6276\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 0.63093\n",
      "Epoch 93/500\n",
      "964/964 [==============================] - 14s 15ms/step - loss: 1.0393 - acc: 0.6153 - val_loss: 1.0245 - val_acc: 0.6209\n",
      "\n",
      "Epoch 00093: val_acc did not improve from 0.63093\n",
      "Epoch 94/500\n",
      "964/964 [==============================] - 14s 14ms/step - loss: 1.0381 - acc: 0.6136 - val_loss: 1.0348 - val_acc: 0.6202\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.63093\n",
      "Epoch 95/500\n",
      "964/964 [==============================] - 14s 14ms/step - loss: 1.0382 - acc: 0.6153 - val_loss: 1.0215 - val_acc: 0.6293\n",
      "\n",
      "Epoch 00095: val_acc did not improve from 0.63093\n",
      "Epoch 96/500\n",
      "964/964 [==============================] - 14s 15ms/step - loss: 1.0380 - acc: 0.6152 - val_loss: 1.0177 - val_acc: 0.6290\n",
      "\n",
      "Epoch 00096: val_acc did not improve from 0.63093\n",
      "Epoch 97/500\n",
      "964/964 [==============================] - 14s 15ms/step - loss: 1.0362 - acc: 0.6156 - val_loss: 1.0128 - val_acc: 0.6286\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 0.63093\n",
      "Epoch 98/500\n",
      "964/964 [==============================] - 14s 15ms/step - loss: 1.0346 - acc: 0.6162 - val_loss: 1.0187 - val_acc: 0.6309\n",
      "\n",
      "Epoch 00098: val_acc did not improve from 0.63093\n",
      "Epoch 99/500\n",
      "964/964 [==============================] - 14s 15ms/step - loss: 1.0364 - acc: 0.6161 - val_loss: 1.0162 - val_acc: 0.6247\n",
      "\n",
      "Epoch 00099: val_acc did not improve from 0.63093\n",
      "Epoch 100/500\n",
      "964/964 [==============================] - 15s 15ms/step - loss: 1.0345 - acc: 0.6147 - val_loss: 1.0083 - val_acc: 0.6290\n",
      "\n",
      "Epoch 00100: val_acc did not improve from 0.63093\n",
      "Epoch 101/500\n",
      "964/964 [==============================] - 14s 15ms/step - loss: 1.0348 - acc: 0.6154 - val_loss: 1.0332 - val_acc: 0.6228\n",
      "\n",
      "Epoch 00101: val_acc did not improve from 0.63093\n",
      "Epoch 102/500\n",
      "964/964 [==============================] - 14s 15ms/step - loss: 1.0358 - acc: 0.6153 - val_loss: 1.0176 - val_acc: 0.6281\n",
      "\n",
      "Epoch 00102: val_acc did not improve from 0.63093\n",
      "Epoch 103/500\n",
      "964/964 [==============================] - 14s 15ms/step - loss: 1.0349 - acc: 0.6153 - val_loss: 1.0081 - val_acc: 0.6298\n",
      "\n",
      "Epoch 00103: val_acc did not improve from 0.63093\n",
      "Epoch 104/500\n",
      "964/964 [==============================] - 15s 16ms/step - loss: 1.0339 - acc: 0.6159 - val_loss: 1.0283 - val_acc: 0.6223\n",
      "\n",
      "Epoch 00104: val_acc did not improve from 0.63093\n",
      "Epoch 105/500\n",
      "964/964 [==============================] - 15s 16ms/step - loss: 1.0338 - acc: 0.6156 - val_loss: 1.0071 - val_acc: 0.6319\n",
      "\n",
      "Epoch 00105: val_acc improved from 0.63093 to 0.63194, saving model to weights_cnn_custom_bn_11.best.hdf5\n",
      "Epoch 106/500\n",
      "964/964 [==============================] - 14s 15ms/step - loss: 1.0348 - acc: 0.6158 - val_loss: 1.0427 - val_acc: 0.6202\n",
      "\n",
      "Epoch 00106: val_acc did not improve from 0.63194\n",
      "Epoch 107/500\n",
      "964/964 [==============================] - 14s 14ms/step - loss: 1.0326 - acc: 0.6155 - val_loss: 1.0450 - val_acc: 0.6188\n",
      "\n",
      "Epoch 00107: val_acc did not improve from 0.63194\n",
      "Epoch 108/500\n",
      "964/964 [==============================] - 14s 14ms/step - loss: 1.0322 - acc: 0.6160 - val_loss: 1.0666 - val_acc: 0.6058\n",
      "\n",
      "Epoch 00108: val_acc did not improve from 0.63194\n",
      "Epoch 109/500\n",
      "964/964 [==============================] - 15s 15ms/step - loss: 1.0337 - acc: 0.6163 - val_loss: 1.0391 - val_acc: 0.6149\n",
      "\n",
      "Epoch 00109: val_acc did not improve from 0.63194\n",
      "Epoch 110/500\n",
      "964/964 [==============================] - 14s 15ms/step - loss: 1.0316 - acc: 0.6159 - val_loss: 1.0490 - val_acc: 0.6107\n",
      "\n",
      "Epoch 00110: val_acc did not improve from 0.63194\n",
      "Epoch 111/500\n",
      "964/964 [==============================] - 14s 15ms/step - loss: 1.0323 - acc: 0.6163 - val_loss: 1.0108 - val_acc: 0.6272\n",
      "\n",
      "Epoch 00111: val_acc did not improve from 0.63194\n",
      "Epoch 112/500\n",
      "964/964 [==============================] - 14s 15ms/step - loss: 1.0309 - acc: 0.6170 - val_loss: 1.0053 - val_acc: 0.6296\n",
      "\n",
      "Epoch 00112: val_acc did not improve from 0.63194\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa82469af28>"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(generator=training_generator,\\\n",
    "                    validation_data=validation_generator,\\\n",
    "                         use_multiprocessing=True,\\\n",
    "                         steps_per_epoch=X_train.shape[0] // 128,\\\n",
    "                        epochs=500,\\\n",
    "                         workers=6,\\\n",
    "                         callbacks=callbacks_list,\\\n",
    "                   class_weight=class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: 1: cp: Argument list too long\r\n"
     ]
    }
   ],
   "source": [
    "%cp train/* all_files/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/notebooks\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(154880, 12, 12, 5)"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model architecture\n",
    "with open('model_architecture.json', 'w') as f:\n",
    "    f.write(model.to_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "\n",
    "# Model reconstruction from JSON file\n",
    "with open('model_architecture.json', 'r') as f:\n",
    "    best_model = model_from_json(f.read())\n",
    "\n",
    "# Load weights into the new model\n",
    "best_model.load_weights('weights_cnn_custom_bn_11.best.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_10 (Conv2D)           (None, 10, 10, 32)        1472      \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 8, 8, 32)          9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 4, 4, 32)          0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 4, 4, 32)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 4, 4, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 2, 2, 64)          18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 1, 1, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 1, 1, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 256)               16640     \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 11)                2827      \n",
      "=================================================================\n",
      "Total params: 49,067\n",
      "Trainable params: 48,875\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "best_model.summary() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_prob = np.ndarray(shape=(X_raw.shape[0],11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(X_raw.shape[0]):\n",
    "    overall_prob[i]=best_model.predict(np.expand_dims(X[i][:,:,:],axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.67879236e-02, 1.12848624e-03, 4.49445006e-03, 2.68102285e-05,\n",
       "       1.52559311e-03, 7.46872649e-03, 1.52359474e-02, 3.18908423e-01,\n",
       "       4.37978357e-01, 1.66027069e-01, 4.51146625e-03, 4.08999808e-03,\n",
       "       1.02688037e-02, 1.54791167e-03])"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall_prob[300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=[]\n",
    "for i in range(X_raw.shape[0]):\n",
    "    y_pred.append(overall_prob[i].argsort()[-1:][::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "154278"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "154278"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(confusion_matrix(labels.tolist(), y_pred),columns = classes.values(),index=classes.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>71</th>\n",
       "      <th>43</th>\n",
       "      <th>11</th>\n",
       "      <th>82</th>\n",
       "      <th>52</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>90</th>\n",
       "      <th>31</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>644</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>5804</td>\n",
       "      <td>4518</td>\n",
       "      <td>467</td>\n",
       "      <td>286</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>363</td>\n",
       "      <td>295</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2335</td>\n",
       "      <td>0</td>\n",
       "      <td>76</td>\n",
       "      <td>63</td>\n",
       "      <td>9</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>92</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>117</td>\n",
       "      <td>485</td>\n",
       "      <td>412</td>\n",
       "      <td>23</td>\n",
       "      <td>50</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>22</td>\n",
       "      <td>51738</td>\n",
       "      <td>3045</td>\n",
       "      <td>237</td>\n",
       "      <td>119</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>460</td>\n",
       "      <td>0</td>\n",
       "      <td>275</td>\n",
       "      <td>67</td>\n",
       "      <td>9831</td>\n",
       "      <td>17509</td>\n",
       "      <td>4224</td>\n",
       "      <td>1321</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>102</td>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>0</td>\n",
       "      <td>1632</td>\n",
       "      <td>8838</td>\n",
       "      <td>7815</td>\n",
       "      <td>4066</td>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>59</td>\n",
       "      <td>0</td>\n",
       "      <td>167</td>\n",
       "      <td>1102</td>\n",
       "      <td>2798</td>\n",
       "      <td>15102</td>\n",
       "      <td>701</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>78</td>\n",
       "      <td>45</td>\n",
       "      <td>2520</td>\n",
       "      <td>1626</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>387</td>\n",
       "      <td>244</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>167</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>184</td>\n",
       "      <td>736</td>\n",
       "      <td>138</td>\n",
       "      <td>284</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     71  43    11   82     52     21    22     23    24  90  31\n",
       "71  644   0     3   18   5804   4518   467    286    32   0   1\n",
       "43    1   0     1    0    363    295    16      1     0   0   0\n",
       "11    1   0  2335    0     76     63     9     15     0   0   0\n",
       "82   92   0     0  117    485    412    23     50    16   0   1\n",
       "52  100   0    40   22  51738   3045   237    119     4   0   0\n",
       "21  460   0   275   67   9831  17509  4224   1321    50   0   1\n",
       "22  102   0   137    0   1632   8838  7815   4066    54   0   0\n",
       "23   18   0    59    0    167   1102  2798  15102   701   0   2\n",
       "24   10   0    21    0     17     78    45   2520  1626   0   1\n",
       "90    3   0     7    0    387    244     1      1     0   0   0\n",
       "31  167   0    13    0    184    736   138    284    14   0   0"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_p = []\n",
    "for i in range(len(y_pred)):\n",
    "    y_p.append(y_pred[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.40      0.05      0.10     11773\n",
      "          1       0.00      0.00      0.00       677\n",
      "          2       0.81      0.93      0.87      2499\n",
      "          3       0.52      0.10      0.16      1196\n",
      "          4       0.73      0.94      0.82     55305\n",
      "          5       0.48      0.52      0.50     33738\n",
      "          6       0.50      0.35      0.41     22644\n",
      "          7       0.64      0.76      0.69     19949\n",
      "          8       0.65      0.38      0.48      4318\n",
      "          9       0.00      0.00      0.00       643\n",
      "         10       0.00      0.00      0.00      1536\n",
      "\n",
      "avg / total       0.59      0.63      0.59    154278\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(labels.tolist(),y_p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1440x1080 with 0 Axes>"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1440x1080 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(20,15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7feaf7127a58>"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x1440 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(20,20))         # Sample figsize in inches\n",
    "sns.heatmap(df, annot=True, linewidths=.5, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170500096/170498071 [==============================] - 36s 0us/step\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import cifar10\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1 = np.ndarray(shape=(10000,197,197,3))\n",
    "for i in range(10000):\n",
    "    X_train1[i]=cv2.resize(X_train[i][:,:,:], dsize=(197, 197), interpolation=cv2.INTER_LANCZOS4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 32, 32, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test1 = np.ndarray(shape=(1000,197,197,3))\n",
    "for i in range(1000):\n",
    "    X_test1[i]=cv2.resize(X_test[i][:,:,:], dsize=(197, 197), interpolation=cv2.INTER_LANCZOS4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classes = 10\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "\n",
    "X_train = X_train1.astype('float32')\n",
    "X_test = X_test1.astype('float32')\n",
    "\n",
    "# subtract mean and normalize\n",
    "mean_image = np.mean(X_train, axis=0)\n",
    "X_train -= mean_image\n",
    "X_test -= mean_image\n",
    "X_train /= 128.\n",
    "X_test /= 128."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ReduceLROnPlateau, CSVLogger, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1), cooldown=0, patience=5, min_lr=0.5e-6)\n",
    "early_stopper = EarlyStopping(min_delta=0.001, patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 10)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test[:1000].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        width_shift_range=0,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=True)  # randomly flip images\n",
    "\n",
    "train_generator = datagen.flow(X_train,Y_train[:10000],batch_size=64)\n",
    "test_generator = datagen.flow(X_test,Y_test[:1000],batch_size=64)\n",
    "\n",
    "    # Compute quantities required for featurewise normalization\n",
    "    # (std, mean, and principal components if ZCA whitening is applied).\n",
    "datagen.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "keras.preprocessing.image.NumpyArrayIterator"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 197, 197, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6NRR (InputLayer)         (None, 197, 197, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 203, 203, 3)  0           input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1_padNRR (ZeroPadding2D)    (None, 203, 203, 3)  0           input_6NRR[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 99, 99, 64)   9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1NRR (Conv2D)               (None, 99, 99, 64)   9472        conv1_padNRR[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn_conv1 (BatchNormalization)   (None, 99, 99, 64)   256         conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "bn_conv1NRR (BatchNormalization (None, 99, 99, 64)   256         conv1NRR[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_197 (Activation)     (None, 99, 99, 64)   0           bn_conv1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_246NRR (Activation)  (None, 99, 99, 64)   0           bn_conv1NRR[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 49, 49, 64)   0           activation_197[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_6NRR (MaxPooling2 (None, 49, 49, 64)   0           activation_246NRR[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2a (Conv2D)         (None, 49, 49, 64)   4160        max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2aNRR (Conv2D)      (None, 49, 49, 64)   4160        max_pooling2d_6NRR[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2a (BatchNormalizati (None, 49, 49, 64)   256         res2a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2aNRR (BatchNormaliz (None, 49, 49, 64)   256         res2a_branch2aNRR[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "activation_198 (Activation)     (None, 49, 49, 64)   0           bn2a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_247NRR (Activation)  (None, 49, 49, 64)   0           bn2a_branch2aNRR[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2b (Conv2D)         (None, 49, 49, 64)   36928       activation_198[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2bNRR (Conv2D)      (None, 49, 49, 64)   36928       activation_247NRR[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2b (BatchNormalizati (None, 49, 49, 64)   256         res2a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2bNRR (BatchNormaliz (None, 49, 49, 64)   256         res2a_branch2bNRR[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "activation_199 (Activation)     (None, 49, 49, 64)   0           bn2a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_248NRR (Activation)  (None, 49, 49, 64)   0           bn2a_branch2bNRR[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2c (Conv2D)         (None, 49, 49, 256)  16640       activation_199[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch1 (Conv2D)          (None, 49, 49, 256)  16640       max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2cNRR (Conv2D)      (None, 49, 49, 256)  16640       activation_248NRR[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch1NRR (Conv2D)       (None, 49, 49, 256)  16640       max_pooling2d_6NRR[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2c (BatchNormalizati (None, 49, 49, 256)  1024        res2a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch1 (BatchNormalizatio (None, 49, 49, 256)  1024        res2a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2cNRR (BatchNormaliz (None, 49, 49, 256)  1024        res2a_branch2cNRR[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch1NRR (BatchNormaliza (None, 49, 49, 256)  1024        res2a_branch1NRR[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_65 (Add)                    (None, 49, 49, 256)  0           bn2a_branch2c[0][0]              \n",
      "                                                                 bn2a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_81NRR (Add)                 (None, 49, 49, 256)  0           bn2a_branch2cNRR[0][0]           \n",
      "                                                                 bn2a_branch1NRR[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_200 (Activation)     (None, 49, 49, 256)  0           add_65[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_249NRR (Activation)  (None, 49, 49, 256)  0           add_81NRR[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2a (Conv2D)         (None, 49, 49, 64)   16448       activation_200[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2aNRR (Conv2D)      (None, 49, 49, 64)   16448       activation_249NRR[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2a (BatchNormalizati (None, 49, 49, 64)   256         res2b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2aNRR (BatchNormaliz (None, 49, 49, 64)   256         res2b_branch2aNRR[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "activation_201 (Activation)     (None, 49, 49, 64)   0           bn2b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_250NRR (Activation)  (None, 49, 49, 64)   0           bn2b_branch2aNRR[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2b (Conv2D)         (None, 49, 49, 64)   36928       activation_201[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2bNRR (Conv2D)      (None, 49, 49, 64)   36928       activation_250NRR[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2b (BatchNormalizati (None, 49, 49, 64)   256         res2b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2bNRR (BatchNormaliz (None, 49, 49, 64)   256         res2b_branch2bNRR[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "activation_202 (Activation)     (None, 49, 49, 64)   0           bn2b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_251NRR (Activation)  (None, 49, 49, 64)   0           bn2b_branch2bNRR[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2c (Conv2D)         (None, 49, 49, 256)  16640       activation_202[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2cNRR (Conv2D)      (None, 49, 49, 256)  16640       activation_251NRR[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2c (BatchNormalizati (None, 49, 49, 256)  1024        res2b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2cNRR (BatchNormaliz (None, 49, 49, 256)  1024        res2b_branch2cNRR[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "add_66 (Add)                    (None, 49, 49, 256)  0           bn2b_branch2c[0][0]              \n",
      "                                                                 activation_200[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_82NRR (Add)                 (None, 49, 49, 256)  0           bn2b_branch2cNRR[0][0]           \n",
      "                                                                 activation_249NRR[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "activation_203 (Activation)     (None, 49, 49, 256)  0           add_66[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_252NRR (Activation)  (None, 49, 49, 256)  0           add_82NRR[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2a (Conv2D)         (None, 49, 49, 64)   16448       activation_203[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2aNRR (Conv2D)      (None, 49, 49, 64)   16448       activation_252NRR[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2a (BatchNormalizati (None, 49, 49, 64)   256         res2c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2aNRR (BatchNormaliz (None, 49, 49, 64)   256         res2c_branch2aNRR[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "activation_204 (Activation)     (None, 49, 49, 64)   0           bn2c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_253NRR (Activation)  (None, 49, 49, 64)   0           bn2c_branch2aNRR[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2b (Conv2D)         (None, 49, 49, 64)   36928       activation_204[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2bNRR (Conv2D)      (None, 49, 49, 64)   36928       activation_253NRR[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2b (BatchNormalizati (None, 49, 49, 64)   256         res2c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2bNRR (BatchNormaliz (None, 49, 49, 64)   256         res2c_branch2bNRR[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "activation_205 (Activation)     (None, 49, 49, 64)   0           bn2c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_254NRR (Activation)  (None, 49, 49, 64)   0           bn2c_branch2bNRR[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2c (Conv2D)         (None, 49, 49, 256)  16640       activation_205[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2cNRR (Conv2D)      (None, 49, 49, 256)  16640       activation_254NRR[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2c (BatchNormalizati (None, 49, 49, 256)  1024        res2c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2cNRR (BatchNormaliz (None, 49, 49, 256)  1024        res2c_branch2cNRR[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "add_67 (Add)                    (None, 49, 49, 256)  0           bn2c_branch2c[0][0]              \n",
      "                                                                 activation_203[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_83NRR (Add)                 (None, 49, 49, 256)  0           bn2c_branch2cNRR[0][0]           \n",
      "                                                                 activation_252NRR[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "activation_206 (Activation)     (None, 49, 49, 256)  0           add_67[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_255NRR (Activation)  (None, 49, 49, 256)  0           add_83NRR[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2a (Conv2D)         (None, 25, 25, 128)  32896       activation_206[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2aNRR (Conv2D)      (None, 25, 25, 128)  32896       activation_255NRR[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2a (BatchNormalizati (None, 25, 25, 128)  512         res3a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2aNRR (BatchNormaliz (None, 25, 25, 128)  512         res3a_branch2aNRR[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "activation_207 (Activation)     (None, 25, 25, 128)  0           bn3a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_256NRR (Activation)  (None, 25, 25, 128)  0           bn3a_branch2aNRR[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2b (Conv2D)         (None, 25, 25, 128)  147584      activation_207[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2bNRR (Conv2D)      (None, 25, 25, 128)  147584      activation_256NRR[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2b (BatchNormalizati (None, 25, 25, 128)  512         res3a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2bNRR (BatchNormaliz (None, 25, 25, 128)  512         res3a_branch2bNRR[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "activation_208 (Activation)     (None, 25, 25, 128)  0           bn3a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_257NRR (Activation)  (None, 25, 25, 128)  0           bn3a_branch2bNRR[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2c (Conv2D)         (None, 25, 25, 512)  66048       activation_208[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch1 (Conv2D)          (None, 25, 25, 512)  131584      activation_206[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2cNRR (Conv2D)      (None, 25, 25, 512)  66048       activation_257NRR[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch1NRR (Conv2D)       (None, 25, 25, 512)  131584      activation_255NRR[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2c (BatchNormalizati (None, 25, 25, 512)  2048        res3a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch1 (BatchNormalizatio (None, 25, 25, 512)  2048        res3a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2cNRR (BatchNormaliz (None, 25, 25, 512)  2048        res3a_branch2cNRR[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch1NRR (BatchNormaliza (None, 25, 25, 512)  2048        res3a_branch1NRR[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_68 (Add)                    (None, 25, 25, 512)  0           bn3a_branch2c[0][0]              \n",
      "                                                                 bn3a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_84NRR (Add)                 (None, 25, 25, 512)  0           bn3a_branch2cNRR[0][0]           \n",
      "                                                                 bn3a_branch1NRR[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_209 (Activation)     (None, 25, 25, 512)  0           add_68[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_258NRR (Activation)  (None, 25, 25, 512)  0           add_84NRR[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2a (Conv2D)         (None, 25, 25, 128)  65664       activation_209[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2aNRR (Conv2D)      (None, 25, 25, 128)  65664       activation_258NRR[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2a (BatchNormalizati (None, 25, 25, 128)  512         res3b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2aNRR (BatchNormaliz (None, 25, 25, 128)  512         res3b_branch2aNRR[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "activation_210 (Activation)     (None, 25, 25, 128)  0           bn3b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_259NRR (Activation)  (None, 25, 25, 128)  0           bn3b_branch2aNRR[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2b (Conv2D)         (None, 25, 25, 128)  147584      activation_210[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2bNRR (Conv2D)      (None, 25, 25, 128)  147584      activation_259NRR[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2b (BatchNormalizati (None, 25, 25, 128)  512         res3b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2bNRR (BatchNormaliz (None, 25, 25, 128)  512         res3b_branch2bNRR[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "activation_211 (Activation)     (None, 25, 25, 128)  0           bn3b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_260NRR (Activation)  (None, 25, 25, 128)  0           bn3b_branch2bNRR[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2c (Conv2D)         (None, 25, 25, 512)  66048       activation_211[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2cNRR (Conv2D)      (None, 25, 25, 512)  66048       activation_260NRR[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2c (BatchNormalizati (None, 25, 25, 512)  2048        res3b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2cNRR (BatchNormaliz (None, 25, 25, 512)  2048        res3b_branch2cNRR[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "add_69 (Add)                    (None, 25, 25, 512)  0           bn3b_branch2c[0][0]              \n",
      "                                                                 activation_209[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_85NRR (Add)                 (None, 25, 25, 512)  0           bn3b_branch2cNRR[0][0]           \n",
      "                                                                 activation_258NRR[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "activation_212 (Activation)     (None, 25, 25, 512)  0           add_69[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_261NRR (Activation)  (None, 25, 25, 512)  0           add_85NRR[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2a (Conv2D)         (None, 25, 25, 128)  65664       activation_212[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2aNRR (Conv2D)      (None, 25, 25, 128)  65664       activation_261NRR[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2a (BatchNormalizati (None, 25, 25, 128)  512         res3c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2aNRR (BatchNormaliz (None, 25, 25, 128)  512         res3c_branch2aNRR[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "activation_213 (Activation)     (None, 25, 25, 128)  0           bn3c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_262NRR (Activation)  (None, 25, 25, 128)  0           bn3c_branch2aNRR[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2b (Conv2D)         (None, 25, 25, 128)  147584      activation_213[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2bNRR (Conv2D)      (None, 25, 25, 128)  147584      activation_262NRR[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2b (BatchNormalizati (None, 25, 25, 128)  512         res3c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2bNRR (BatchNormaliz (None, 25, 25, 128)  512         res3c_branch2bNRR[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "activation_214 (Activation)     (None, 25, 25, 128)  0           bn3c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_263NRR (Activation)  (None, 25, 25, 128)  0           bn3c_branch2bNRR[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2c (Conv2D)         (None, 25, 25, 512)  66048       activation_214[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2cNRR (Conv2D)      (None, 25, 25, 512)  66048       activation_263NRR[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2c (BatchNormalizati (None, 25, 25, 512)  2048        res3c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2cNRR (BatchNormaliz (None, 25, 25, 512)  2048        res3c_branch2cNRR[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "add_70 (Add)                    (None, 25, 25, 512)  0           bn3c_branch2c[0][0]              \n",
      "                                                                 activation_212[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_86NRR (Add)                 (None, 25, 25, 512)  0           bn3c_branch2cNRR[0][0]           \n",
      "                                                                 activation_261NRR[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "activation_215 (Activation)     (None, 25, 25, 512)  0           add_70[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_264NRR (Activation)  (None, 25, 25, 512)  0           add_86NRR[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2a (Conv2D)         (None, 25, 25, 128)  65664       activation_215[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2aNRR (Conv2D)      (None, 25, 25, 128)  65664       activation_264NRR[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2a (BatchNormalizati (None, 25, 25, 128)  512         res3d_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2aNRR (BatchNormaliz (None, 25, 25, 128)  512         res3d_branch2aNRR[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "activation_216 (Activation)     (None, 25, 25, 128)  0           bn3d_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_265NRR (Activation)  (None, 25, 25, 128)  0           bn3d_branch2aNRR[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2b (Conv2D)         (None, 25, 25, 128)  147584      activation_216[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2bNRR (Conv2D)      (None, 25, 25, 128)  147584      activation_265NRR[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2b (BatchNormalizati (None, 25, 25, 128)  512         res3d_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2bNRR (BatchNormaliz (None, 25, 25, 128)  512         res3d_branch2bNRR[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "activation_217 (Activation)     (None, 25, 25, 128)  0           bn3d_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_266NRR (Activation)  (None, 25, 25, 128)  0           bn3d_branch2bNRR[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2c (Conv2D)         (None, 25, 25, 512)  66048       activation_217[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2cNRR (Conv2D)      (None, 25, 25, 512)  66048       activation_266NRR[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2c (BatchNormalizati (None, 25, 25, 512)  2048        res3d_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2cNRR (BatchNormaliz (None, 25, 25, 512)  2048        res3d_branch2cNRR[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "add_71 (Add)                    (None, 25, 25, 512)  0           bn3d_branch2c[0][0]              \n",
      "                                                                 activation_215[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_87NRR (Add)                 (None, 25, 25, 512)  0           bn3d_branch2cNRR[0][0]           \n",
      "                                                                 activation_264NRR[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "activation_218 (Activation)     (None, 25, 25, 512)  0           add_71[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_267NRR (Activation)  (None, 25, 25, 512)  0           add_87NRR[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2a (Conv2D)         (None, 13, 13, 256)  131328      activation_218[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2aNRR (Conv2D)      (None, 13, 13, 256)  131328      activation_267NRR[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2a (BatchNormalizati (None, 13, 13, 256)  1024        res4a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2aNRR (BatchNormaliz (None, 13, 13, 256)  1024        res4a_branch2aNRR[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "activation_219 (Activation)     (None, 13, 13, 256)  0           bn4a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_268NRR (Activation)  (None, 13, 13, 256)  0           bn4a_branch2aNRR[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2b (Conv2D)         (None, 13, 13, 256)  590080      activation_219[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2bNRR (Conv2D)      (None, 13, 13, 256)  590080      activation_268NRR[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2b (BatchNormalizati (None, 13, 13, 256)  1024        res4a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2bNRR (BatchNormaliz (None, 13, 13, 256)  1024        res4a_branch2bNRR[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "activation_220 (Activation)     (None, 13, 13, 256)  0           bn4a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_269NRR (Activation)  (None, 13, 13, 256)  0           bn4a_branch2bNRR[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2c (Conv2D)         (None, 13, 13, 1024) 263168      activation_220[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch1 (Conv2D)          (None, 13, 13, 1024) 525312      activation_218[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2cNRR (Conv2D)      (None, 13, 13, 1024) 263168      activation_269NRR[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch1NRR (Conv2D)       (None, 13, 13, 1024) 525312      activation_267NRR[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2c (BatchNormalizati (None, 13, 13, 1024) 4096        res4a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch1 (BatchNormalizatio (None, 13, 13, 1024) 4096        res4a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2cNRR (BatchNormaliz (None, 13, 13, 1024) 4096        res4a_branch2cNRR[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch1NRR (BatchNormaliza (None, 13, 13, 1024) 4096        res4a_branch1NRR[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_72 (Add)                    (None, 13, 13, 1024) 0           bn4a_branch2c[0][0]              \n",
      "                                                                 bn4a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_88NRR (Add)                 (None, 13, 13, 1024) 0           bn4a_branch2cNRR[0][0]           \n",
      "                                                                 bn4a_branch1NRR[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_221 (Activation)     (None, 13, 13, 1024) 0           add_72[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_270NRR (Activation)  (None, 13, 13, 1024) 0           add_88NRR[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2a (Conv2D)         (None, 13, 13, 256)  262400      activation_221[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2aNRR (Conv2D)      (None, 13, 13, 256)  262400      activation_270NRR[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2a (BatchNormalizati (None, 13, 13, 256)  1024        res4b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2aNRR (BatchNormaliz (None, 13, 13, 256)  1024        res4b_branch2aNRR[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "activation_222 (Activation)     (None, 13, 13, 256)  0           bn4b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_271NRR (Activation)  (None, 13, 13, 256)  0           bn4b_branch2aNRR[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2b (Conv2D)         (None, 13, 13, 256)  590080      activation_222[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2bNRR (Conv2D)      (None, 13, 13, 256)  590080      activation_271NRR[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2b (BatchNormalizati (None, 13, 13, 256)  1024        res4b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2bNRR (BatchNormaliz (None, 13, 13, 256)  1024        res4b_branch2bNRR[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "activation_223 (Activation)     (None, 13, 13, 256)  0           bn4b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_272NRR (Activation)  (None, 13, 13, 256)  0           bn4b_branch2bNRR[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2c (Conv2D)         (None, 13, 13, 1024) 263168      activation_223[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2cNRR (Conv2D)      (None, 13, 13, 1024) 263168      activation_272NRR[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2c (BatchNormalizati (None, 13, 13, 1024) 4096        res4b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2cNRR (BatchNormaliz (None, 13, 13, 1024) 4096        res4b_branch2cNRR[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "add_73 (Add)                    (None, 13, 13, 1024) 0           bn4b_branch2c[0][0]              \n",
      "                                                                 activation_221[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_89NRR (Add)                 (None, 13, 13, 1024) 0           bn4b_branch2cNRR[0][0]           \n",
      "                                                                 activation_270NRR[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "activation_224 (Activation)     (None, 13, 13, 1024) 0           add_73[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_273NRR (Activation)  (None, 13, 13, 1024) 0           add_89NRR[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2a (Conv2D)         (None, 13, 13, 256)  262400      activation_224[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2aNRR (Conv2D)      (None, 13, 13, 256)  262400      activation_273NRR[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2a (BatchNormalizati (None, 13, 13, 256)  1024        res4c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2aNRR (BatchNormaliz (None, 13, 13, 256)  1024        res4c_branch2aNRR[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "activation_225 (Activation)     (None, 13, 13, 256)  0           bn4c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_274NRR (Activation)  (None, 13, 13, 256)  0           bn4c_branch2aNRR[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2b (Conv2D)         (None, 13, 13, 256)  590080      activation_225[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2bNRR (Conv2D)      (None, 13, 13, 256)  590080      activation_274NRR[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2b (BatchNormalizati (None, 13, 13, 256)  1024        res4c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2bNRR (BatchNormaliz (None, 13, 13, 256)  1024        res4c_branch2bNRR[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "activation_226 (Activation)     (None, 13, 13, 256)  0           bn4c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_275NRR (Activation)  (None, 13, 13, 256)  0           bn4c_branch2bNRR[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2c (Conv2D)         (None, 13, 13, 1024) 263168      activation_226[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2cNRR (Conv2D)      (None, 13, 13, 1024) 263168      activation_275NRR[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2c (BatchNormalizati (None, 13, 13, 1024) 4096        res4c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2cNRR (BatchNormaliz (None, 13, 13, 1024) 4096        res4c_branch2cNRR[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "add_74 (Add)                    (None, 13, 13, 1024) 0           bn4c_branch2c[0][0]              \n",
      "                                                                 activation_224[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_90NRR (Add)                 (None, 13, 13, 1024) 0           bn4c_branch2cNRR[0][0]           \n",
      "                                                                 activation_273NRR[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "activation_227 (Activation)     (None, 13, 13, 1024) 0           add_74[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_276NRR (Activation)  (None, 13, 13, 1024) 0           add_90NRR[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2a (Conv2D)         (None, 13, 13, 256)  262400      activation_227[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2aNRR (Conv2D)      (None, 13, 13, 256)  262400      activation_276NRR[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2a (BatchNormalizati (None, 13, 13, 256)  1024        res4d_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2aNRR (BatchNormaliz (None, 13, 13, 256)  1024        res4d_branch2aNRR[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "activation_228 (Activation)     (None, 13, 13, 256)  0           bn4d_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_277NRR (Activation)  (None, 13, 13, 256)  0           bn4d_branch2aNRR[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2b (Conv2D)         (None, 13, 13, 256)  590080      activation_228[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2bNRR (Conv2D)      (None, 13, 13, 256)  590080      activation_277NRR[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2b (BatchNormalizati (None, 13, 13, 256)  1024        res4d_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2bNRR (BatchNormaliz (None, 13, 13, 256)  1024        res4d_branch2bNRR[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "activation_229 (Activation)     (None, 13, 13, 256)  0           bn4d_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_278NRR (Activation)  (None, 13, 13, 256)  0           bn4d_branch2bNRR[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2c (Conv2D)         (None, 13, 13, 1024) 263168      activation_229[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2cNRR (Conv2D)      (None, 13, 13, 1024) 263168      activation_278NRR[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2c (BatchNormalizati (None, 13, 13, 1024) 4096        res4d_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2cNRR (BatchNormaliz (None, 13, 13, 1024) 4096        res4d_branch2cNRR[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "add_75 (Add)                    (None, 13, 13, 1024) 0           bn4d_branch2c[0][0]              \n",
      "                                                                 activation_227[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_91NRR (Add)                 (None, 13, 13, 1024) 0           bn4d_branch2cNRR[0][0]           \n",
      "                                                                 activation_276NRR[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "activation_230 (Activation)     (None, 13, 13, 1024) 0           add_75[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_279NRR (Activation)  (None, 13, 13, 1024) 0           add_91NRR[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2a (Conv2D)         (None, 13, 13, 256)  262400      activation_230[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2aNRR (Conv2D)      (None, 13, 13, 256)  262400      activation_279NRR[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2a (BatchNormalizati (None, 13, 13, 256)  1024        res4e_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2aNRR (BatchNormaliz (None, 13, 13, 256)  1024        res4e_branch2aNRR[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "activation_231 (Activation)     (None, 13, 13, 256)  0           bn4e_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_280NRR (Activation)  (None, 13, 13, 256)  0           bn4e_branch2aNRR[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2b (Conv2D)         (None, 13, 13, 256)  590080      activation_231[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2bNRR (Conv2D)      (None, 13, 13, 256)  590080      activation_280NRR[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2b (BatchNormalizati (None, 13, 13, 256)  1024        res4e_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2bNRR (BatchNormaliz (None, 13, 13, 256)  1024        res4e_branch2bNRR[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "activation_232 (Activation)     (None, 13, 13, 256)  0           bn4e_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_281NRR (Activation)  (None, 13, 13, 256)  0           bn4e_branch2bNRR[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2c (Conv2D)         (None, 13, 13, 1024) 263168      activation_232[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2cNRR (Conv2D)      (None, 13, 13, 1024) 263168      activation_281NRR[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2c (BatchNormalizati (None, 13, 13, 1024) 4096        res4e_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2cNRR (BatchNormaliz (None, 13, 13, 1024) 4096        res4e_branch2cNRR[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "add_76 (Add)                    (None, 13, 13, 1024) 0           bn4e_branch2c[0][0]              \n",
      "                                                                 activation_230[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_92NRR (Add)                 (None, 13, 13, 1024) 0           bn4e_branch2cNRR[0][0]           \n",
      "                                                                 activation_279NRR[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "activation_233 (Activation)     (None, 13, 13, 1024) 0           add_76[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_282NRR (Activation)  (None, 13, 13, 1024) 0           add_92NRR[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2a (Conv2D)         (None, 13, 13, 256)  262400      activation_233[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2aNRR (Conv2D)      (None, 13, 13, 256)  262400      activation_282NRR[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2a (BatchNormalizati (None, 13, 13, 256)  1024        res4f_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2aNRR (BatchNormaliz (None, 13, 13, 256)  1024        res4f_branch2aNRR[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "activation_234 (Activation)     (None, 13, 13, 256)  0           bn4f_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_283NRR (Activation)  (None, 13, 13, 256)  0           bn4f_branch2aNRR[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2b (Conv2D)         (None, 13, 13, 256)  590080      activation_234[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2bNRR (Conv2D)      (None, 13, 13, 256)  590080      activation_283NRR[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2b (BatchNormalizati (None, 13, 13, 256)  1024        res4f_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2bNRR (BatchNormaliz (None, 13, 13, 256)  1024        res4f_branch2bNRR[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "activation_235 (Activation)     (None, 13, 13, 256)  0           bn4f_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_284NRR (Activation)  (None, 13, 13, 256)  0           bn4f_branch2bNRR[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2c (Conv2D)         (None, 13, 13, 1024) 263168      activation_235[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2cNRR (Conv2D)      (None, 13, 13, 1024) 263168      activation_284NRR[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2c (BatchNormalizati (None, 13, 13, 1024) 4096        res4f_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2cNRR (BatchNormaliz (None, 13, 13, 1024) 4096        res4f_branch2cNRR[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "add_77 (Add)                    (None, 13, 13, 1024) 0           bn4f_branch2c[0][0]              \n",
      "                                                                 activation_233[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_93NRR (Add)                 (None, 13, 13, 1024) 0           bn4f_branch2cNRR[0][0]           \n",
      "                                                                 activation_282NRR[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "activation_236 (Activation)     (None, 13, 13, 1024) 0           add_77[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_285NRR (Activation)  (None, 13, 13, 1024) 0           add_93NRR[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2a (Conv2D)         (None, 7, 7, 512)    524800      activation_236[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2aNRR (Conv2D)      (None, 7, 7, 512)    524800      activation_285NRR[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2a (BatchNormalizati (None, 7, 7, 512)    2048        res5a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2aNRR (BatchNormaliz (None, 7, 7, 512)    2048        res5a_branch2aNRR[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "activation_237 (Activation)     (None, 7, 7, 512)    0           bn5a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_286NRR (Activation)  (None, 7, 7, 512)    0           bn5a_branch2aNRR[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2b (Conv2D)         (None, 7, 7, 512)    2359808     activation_237[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2bNRR (Conv2D)      (None, 7, 7, 512)    2359808     activation_286NRR[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2b (BatchNormalizati (None, 7, 7, 512)    2048        res5a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2bNRR (BatchNormaliz (None, 7, 7, 512)    2048        res5a_branch2bNRR[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "activation_238 (Activation)     (None, 7, 7, 512)    0           bn5a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_287NRR (Activation)  (None, 7, 7, 512)    0           bn5a_branch2bNRR[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2c (Conv2D)         (None, 7, 7, 2048)   1050624     activation_238[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch1 (Conv2D)          (None, 7, 7, 2048)   2099200     activation_236[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2cNRR (Conv2D)      (None, 7, 7, 2048)   1050624     activation_287NRR[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch1NRR (Conv2D)       (None, 7, 7, 2048)   2099200     activation_285NRR[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2c (BatchNormalizati (None, 7, 7, 2048)   8192        res5a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch1 (BatchNormalizatio (None, 7, 7, 2048)   8192        res5a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2cNRR (BatchNormaliz (None, 7, 7, 2048)   8192        res5a_branch2cNRR[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch1NRR (BatchNormaliza (None, 7, 7, 2048)   8192        res5a_branch1NRR[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_78 (Add)                    (None, 7, 7, 2048)   0           bn5a_branch2c[0][0]              \n",
      "                                                                 bn5a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_94NRR (Add)                 (None, 7, 7, 2048)   0           bn5a_branch2cNRR[0][0]           \n",
      "                                                                 bn5a_branch1NRR[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_239 (Activation)     (None, 7, 7, 2048)   0           add_78[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_288NRR (Activation)  (None, 7, 7, 2048)   0           add_94NRR[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2a (Conv2D)         (None, 7, 7, 512)    1049088     activation_239[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2aNRR (Conv2D)      (None, 7, 7, 512)    1049088     activation_288NRR[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2a (BatchNormalizati (None, 7, 7, 512)    2048        res5b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2aNRR (BatchNormaliz (None, 7, 7, 512)    2048        res5b_branch2aNRR[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "activation_240 (Activation)     (None, 7, 7, 512)    0           bn5b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_289NRR (Activation)  (None, 7, 7, 512)    0           bn5b_branch2aNRR[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2b (Conv2D)         (None, 7, 7, 512)    2359808     activation_240[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2bNRR (Conv2D)      (None, 7, 7, 512)    2359808     activation_289NRR[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2b (BatchNormalizati (None, 7, 7, 512)    2048        res5b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2bNRR (BatchNormaliz (None, 7, 7, 512)    2048        res5b_branch2bNRR[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "activation_241 (Activation)     (None, 7, 7, 512)    0           bn5b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_290NRR (Activation)  (None, 7, 7, 512)    0           bn5b_branch2bNRR[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2c (Conv2D)         (None, 7, 7, 2048)   1050624     activation_241[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2cNRR (Conv2D)      (None, 7, 7, 2048)   1050624     activation_290NRR[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2c (BatchNormalizati (None, 7, 7, 2048)   8192        res5b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2cNRR (BatchNormaliz (None, 7, 7, 2048)   8192        res5b_branch2cNRR[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "add_79 (Add)                    (None, 7, 7, 2048)   0           bn5b_branch2c[0][0]              \n",
      "                                                                 activation_239[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_95NRR (Add)                 (None, 7, 7, 2048)   0           bn5b_branch2cNRR[0][0]           \n",
      "                                                                 activation_288NRR[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "activation_242 (Activation)     (None, 7, 7, 2048)   0           add_79[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_291NRR (Activation)  (None, 7, 7, 2048)   0           add_95NRR[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2a (Conv2D)         (None, 7, 7, 512)    1049088     activation_242[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2aNRR (Conv2D)      (None, 7, 7, 512)    1049088     activation_291NRR[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bn5c_branch2a (BatchNormalizati (None, 7, 7, 512)    2048        res5c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5c_branch2aNRR (BatchNormaliz (None, 7, 7, 512)    2048        res5c_branch2aNRR[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "activation_243 (Activation)     (None, 7, 7, 512)    0           bn5c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_292NRR (Activation)  (None, 7, 7, 512)    0           bn5c_branch2aNRR[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2b (Conv2D)         (None, 7, 7, 512)    2359808     activation_243[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2bNRR (Conv2D)      (None, 7, 7, 512)    2359808     activation_292NRR[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bn5c_branch2b (BatchNormalizati (None, 7, 7, 512)    2048        res5c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5c_branch2bNRR (BatchNormaliz (None, 7, 7, 512)    2048        res5c_branch2bNRR[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "activation_244 (Activation)     (None, 7, 7, 512)    0           bn5c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_293NRR (Activation)  (None, 7, 7, 512)    0           bn5c_branch2bNRR[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2c (Conv2D)         (None, 7, 7, 2048)   1050624     activation_244[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2cNRR (Conv2D)      (None, 7, 7, 2048)   1050624     activation_293NRR[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bn5c_branch2c (BatchNormalizati (None, 7, 7, 2048)   8192        res5c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5c_branch2cNRR (BatchNormaliz (None, 7, 7, 2048)   8192        res5c_branch2cNRR[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "add_80 (Add)                    (None, 7, 7, 2048)   0           bn5c_branch2c[0][0]              \n",
      "                                                                 activation_242[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_96NRR (Add)                 (None, 7, 7, 2048)   0           bn5c_branch2cNRR[0][0]           \n",
      "                                                                 activation_291NRR[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "activation_245 (Activation)     (None, 7, 7, 2048)   0           add_80[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_294NRR (Activation)  (None, 7, 7, 2048)   0           add_96NRR[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "avg_pool (AveragePooling2D)     (None, 1, 1, 2048)   0           activation_245[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "avg_poolNRR (AveragePooling2D)  (None, 1, 1, 2048)   0           activation_294NRR[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 1, 1, 4096)   0           avg_pool[0][0]                   \n",
      "                                                                 avg_poolNRR[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 1, 1, 4096)   16781312    concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 1, 1, 10)     40970       dense_7[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 63,997,706\n",
      "Trainable params: 16,822,282\n",
      "Non-trainable params: 47,175,424\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "final_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_RGB = ResNet50(weights='imagenet', include_top=False, input_shape=(197,197,3))\n",
    "\n",
    "for layer in model_RGB.layers:\n",
    "    layer.trainable = False\n",
    "    \n",
    "x = Dense(2048, activation='relu')(model_RGB.output)\n",
    "predictions = Dense(10, activation='softmax')(x)\n",
    "\n",
    "final_model = Model(inputs=model_RGB.input, outputs=predictions)\n",
    "\n",
    "sgd = optimizers.SGD(lr=0.0001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "final_model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "10000/10000 [==============================] - 41s 4ms/step - loss: 2.0125 - acc: 0.3267 - val_loss: 2.4706 - val_acc: 0.0980\n",
      "Epoch 2/10\n",
      "10000/10000 [==============================] - 31s 3ms/step - loss: 1.4790 - acc: 0.5968 - val_loss: 2.5927 - val_acc: 0.1030\n",
      "Epoch 3/10\n",
      "10000/10000 [==============================] - 30s 3ms/step - loss: 1.2071 - acc: 0.6715 - val_loss: 2.6795 - val_acc: 0.1030\n",
      "Epoch 4/10\n",
      "10000/10000 [==============================] - 30s 3ms/step - loss: 1.0563 - acc: 0.7019 - val_loss: 2.8048 - val_acc: 0.1030\n",
      "Epoch 5/10\n",
      "10000/10000 [==============================] - 30s 3ms/step - loss: 0.9579 - acc: 0.7226 - val_loss: 2.8559 - val_acc: 0.1030\n",
      "Epoch 6/10\n",
      " 5024/10000 [==============>...............] - ETA: 13s - loss: 0.9013 - acc: 0.7337"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-125-7c867440fde6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m               \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m               \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m               callbacks=[lr_reducer, early_stopper])\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1234\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1236\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1237\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2480\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2481\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2482\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2483\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "final_model.fit(X_train, np.expand_dims(np.expand_dims(Y_train[:10000],axis=1),axis=1),\n",
    "              batch_size=32,\n",
    "              epochs=10,\n",
    "              validation_data=(X_test, np.expand_dims(np.expand_dims(Y_test[:1000],axis=1),axis=1)),\n",
    "              shuffle=True,\n",
    "              callbacks=[lr_reducer, early_stopper])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:6: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(([array([[..., epochs=50, verbose=1, callbacks=[<keras.ca..., max_queue_size=100, validation_data=([array([[..., steps_per_epoch=100)`\n",
      "  \n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'tuple' object is not an iterator",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-78-58c2dbe74120>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_q_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m                     callbacks=[lr_reducer, early_stopper])\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2192\u001b[0m                 \u001b[0mbatch_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2193\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2194\u001b[0;31m                     \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2196\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__len__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 793\u001b[0;31m                 \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/six.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    691\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 693\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    694\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36m_data_generator_task\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    656\u001b[0m                             \u001b[0;31m# => Serialize calls to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m                             \u001b[0;31m# infinite iterator/generator's next() function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 658\u001b[0;31m                             \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    659\u001b[0m                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'tuple' object is not an iterator"
     ]
    }
   ],
   "source": [
    "    # Fit the model on the batches generated by datagen.flow().\n",
    "final_model.fit_generator(([X_train,X_train],np.expand_dims(np.expand_dims(Y_train[:10000],axis=1),axis=1)),\n",
    "                        steps_per_epoch=100,\n",
    "                        validation_data=([X_test,X_test],np.expand_dims(np.expand_dims(Y_test[:1000],axis=1),axis=1)),\n",
    "                        epochs=50, verbose=1, max_q_size=100,\n",
    "                        callbacks=[lr_reducer, early_stopper])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1 = np.ndarray(shape=((15488, 216, 216, 3)))\n",
    "for i in range(0,15488):\n",
    "    X_train1[i]=cv2.resize(X_train[i][:,:,:3], dsize=(216, 216), interpolation=cv2.INTER_NEAREST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables\n",
    "batch_size = 128\n",
    "nb_epoch = 50\n",
    "steps_per_epoch = 100\n",
    "window_size = X_train1.shape[1]\n",
    "nb_classes = Y_train.shape[1]\n",
    "nb_channels = X_train1.shape[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test1 = np.ndarray(shape=((1548, 216, 216, 3)))\n",
    "for i in range(0,1548):\n",
    "    X_test1[i]=cv2.resize(X_test[i][:,:,:3], dsize=(216, 216), interpolation=cv2.INTER_NEAREST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test=np.expand_dims(np.expand_dims(Y_test,axis=1),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1548, 1, 1, 14)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test[:1548].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 216, 216, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 222, 222, 3)  0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 108, 108, 64) 9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bn_conv1 (BatchNormalization)   (None, 108, 108, 64) 256         conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 108, 108, 64) 0           bn_conv1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 53, 53, 64)   0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2a (Conv2D)         (None, 53, 53, 64)   4160        max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2a (BatchNormalizati (None, 53, 53, 64)   256         res2a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 53, 53, 64)   0           bn2a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2b (Conv2D)         (None, 53, 53, 64)   36928       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2b (BatchNormalizati (None, 53, 53, 64)   256         res2a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 53, 53, 64)   0           bn2a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2c (Conv2D)         (None, 53, 53, 256)  16640       activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch1 (Conv2D)          (None, 53, 53, 256)  16640       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2c (BatchNormalizati (None, 53, 53, 256)  1024        res2a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch1 (BatchNormalizatio (None, 53, 53, 256)  1024        res2a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 53, 53, 256)  0           bn2a_branch2c[0][0]              \n",
      "                                                                 bn2a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 53, 53, 256)  0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2a (Conv2D)         (None, 53, 53, 64)   16448       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2a (BatchNormalizati (None, 53, 53, 64)   256         res2b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 53, 53, 64)   0           bn2b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2b (Conv2D)         (None, 53, 53, 64)   36928       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2b (BatchNormalizati (None, 53, 53, 64)   256         res2b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 53, 53, 64)   0           bn2b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2c (Conv2D)         (None, 53, 53, 256)  16640       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2c (BatchNormalizati (None, 53, 53, 256)  1024        res2b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 53, 53, 256)  0           bn2b_branch2c[0][0]              \n",
      "                                                                 activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 53, 53, 256)  0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2a (Conv2D)         (None, 53, 53, 64)   16448       activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2a (BatchNormalizati (None, 53, 53, 64)   256         res2c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 53, 53, 64)   0           bn2c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2b (Conv2D)         (None, 53, 53, 64)   36928       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2b (BatchNormalizati (None, 53, 53, 64)   256         res2c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 53, 53, 64)   0           bn2c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2c (Conv2D)         (None, 53, 53, 256)  16640       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2c (BatchNormalizati (None, 53, 53, 256)  1024        res2c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 53, 53, 256)  0           bn2c_branch2c[0][0]              \n",
      "                                                                 activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 53, 53, 256)  0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2a (Conv2D)         (None, 27, 27, 128)  32896       activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2a (BatchNormalizati (None, 27, 27, 128)  512         res3a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 27, 27, 128)  0           bn3a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2b (Conv2D)         (None, 27, 27, 128)  147584      activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2b (BatchNormalizati (None, 27, 27, 128)  512         res3a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 27, 27, 128)  0           bn3a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2c (Conv2D)         (None, 27, 27, 512)  66048       activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch1 (Conv2D)          (None, 27, 27, 512)  131584      activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2c (BatchNormalizati (None, 27, 27, 512)  2048        res3a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch1 (BatchNormalizatio (None, 27, 27, 512)  2048        res3a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 27, 27, 512)  0           bn3a_branch2c[0][0]              \n",
      "                                                                 bn3a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 27, 27, 512)  0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2a (Conv2D)         (None, 27, 27, 128)  65664       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2a (BatchNormalizati (None, 27, 27, 128)  512         res3b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 27, 27, 128)  0           bn3b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2b (Conv2D)         (None, 27, 27, 128)  147584      activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2b (BatchNormalizati (None, 27, 27, 128)  512         res3b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 27, 27, 128)  0           bn3b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2c (Conv2D)         (None, 27, 27, 512)  66048       activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2c (BatchNormalizati (None, 27, 27, 512)  2048        res3b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 27, 27, 512)  0           bn3b_branch2c[0][0]              \n",
      "                                                                 activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 27, 27, 512)  0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2a (Conv2D)         (None, 27, 27, 128)  65664       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2a (BatchNormalizati (None, 27, 27, 128)  512         res3c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 27, 27, 128)  0           bn3c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2b (Conv2D)         (None, 27, 27, 128)  147584      activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2b (BatchNormalizati (None, 27, 27, 128)  512         res3c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 27, 27, 128)  0           bn3c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2c (Conv2D)         (None, 27, 27, 512)  66048       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2c (BatchNormalizati (None, 27, 27, 512)  2048        res3c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 27, 27, 512)  0           bn3c_branch2c[0][0]              \n",
      "                                                                 activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 27, 27, 512)  0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2a (Conv2D)         (None, 27, 27, 128)  65664       activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2a (BatchNormalizati (None, 27, 27, 128)  512         res3d_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 27, 27, 128)  0           bn3d_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2b (Conv2D)         (None, 27, 27, 128)  147584      activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2b (BatchNormalizati (None, 27, 27, 128)  512         res3d_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 27, 27, 128)  0           bn3d_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2c (Conv2D)         (None, 27, 27, 512)  66048       activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2c (BatchNormalizati (None, 27, 27, 512)  2048        res3d_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 27, 27, 512)  0           bn3d_branch2c[0][0]              \n",
      "                                                                 activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 27, 27, 512)  0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2a (Conv2D)         (None, 14, 14, 256)  131328      activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2a (BatchNormalizati (None, 14, 14, 256)  1024        res4a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 14, 14, 256)  0           bn4a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2b (Conv2D)         (None, 14, 14, 256)  590080      activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2b (BatchNormalizati (None, 14, 14, 256)  1024        res4a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 14, 14, 256)  0           bn4a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2c (Conv2D)         (None, 14, 14, 1024) 263168      activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch1 (Conv2D)          (None, 14, 14, 1024) 525312      activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2c (BatchNormalizati (None, 14, 14, 1024) 4096        res4a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch1 (BatchNormalizatio (None, 14, 14, 1024) 4096        res4a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 14, 14, 1024) 0           bn4a_branch2c[0][0]              \n",
      "                                                                 bn4a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 14, 14, 1024) 0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2a (Conv2D)         (None, 14, 14, 256)  262400      activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2a (BatchNormalizati (None, 14, 14, 256)  1024        res4b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 14, 14, 256)  0           bn4b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2b (Conv2D)         (None, 14, 14, 256)  590080      activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2b (BatchNormalizati (None, 14, 14, 256)  1024        res4b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 14, 14, 256)  0           bn4b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2c (Conv2D)         (None, 14, 14, 1024) 263168      activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2c (BatchNormalizati (None, 14, 14, 1024) 4096        res4b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 14, 14, 1024) 0           bn4b_branch2c[0][0]              \n",
      "                                                                 activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 14, 14, 1024) 0           add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2a (Conv2D)         (None, 14, 14, 256)  262400      activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2a (BatchNormalizati (None, 14, 14, 256)  1024        res4c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 14, 14, 256)  0           bn4c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2b (Conv2D)         (None, 14, 14, 256)  590080      activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2b (BatchNormalizati (None, 14, 14, 256)  1024        res4c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 14, 14, 256)  0           bn4c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2c (Conv2D)         (None, 14, 14, 1024) 263168      activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2c (BatchNormalizati (None, 14, 14, 1024) 4096        res4c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 14, 14, 1024) 0           bn4c_branch2c[0][0]              \n",
      "                                                                 activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 14, 14, 1024) 0           add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2a (Conv2D)         (None, 14, 14, 256)  262400      activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2a (BatchNormalizati (None, 14, 14, 256)  1024        res4d_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 14, 14, 256)  0           bn4d_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2b (Conv2D)         (None, 14, 14, 256)  590080      activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2b (BatchNormalizati (None, 14, 14, 256)  1024        res4d_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 14, 14, 256)  0           bn4d_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2c (Conv2D)         (None, 14, 14, 1024) 263168      activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2c (BatchNormalizati (None, 14, 14, 1024) 4096        res4d_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 14, 14, 1024) 0           bn4d_branch2c[0][0]              \n",
      "                                                                 activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 14, 14, 1024) 0           add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2a (Conv2D)         (None, 14, 14, 256)  262400      activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2a (BatchNormalizati (None, 14, 14, 256)  1024        res4e_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 14, 14, 256)  0           bn4e_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2b (Conv2D)         (None, 14, 14, 256)  590080      activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2b (BatchNormalizati (None, 14, 14, 256)  1024        res4e_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 14, 14, 256)  0           bn4e_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2c (Conv2D)         (None, 14, 14, 1024) 263168      activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2c (BatchNormalizati (None, 14, 14, 1024) 4096        res4e_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 14, 14, 1024) 0           bn4e_branch2c[0][0]              \n",
      "                                                                 activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 14, 14, 1024) 0           add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2a (Conv2D)         (None, 14, 14, 256)  262400      activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2a (BatchNormalizati (None, 14, 14, 256)  1024        res4f_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 14, 14, 256)  0           bn4f_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2b (Conv2D)         (None, 14, 14, 256)  590080      activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2b (BatchNormalizati (None, 14, 14, 256)  1024        res4f_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 14, 14, 256)  0           bn4f_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2c (Conv2D)         (None, 14, 14, 1024) 263168      activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2c (BatchNormalizati (None, 14, 14, 1024) 4096        res4f_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 14, 14, 1024) 0           bn4f_branch2c[0][0]              \n",
      "                                                                 activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 14, 14, 1024) 0           add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2a (Conv2D)         (None, 7, 7, 512)    524800      activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2a (BatchNormalizati (None, 7, 7, 512)    2048        res5a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 7, 7, 512)    0           bn5a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2b (Conv2D)         (None, 7, 7, 512)    2359808     activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2b (BatchNormalizati (None, 7, 7, 512)    2048        res5a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 7, 7, 512)    0           bn5a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2c (Conv2D)         (None, 7, 7, 2048)   1050624     activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch1 (Conv2D)          (None, 7, 7, 2048)   2099200     activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2c (BatchNormalizati (None, 7, 7, 2048)   8192        res5a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch1 (BatchNormalizatio (None, 7, 7, 2048)   8192        res5a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, 7, 7, 2048)   0           bn5a_branch2c[0][0]              \n",
      "                                                                 bn5a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 7, 7, 2048)   0           add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2a (Conv2D)         (None, 7, 7, 512)    1049088     activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2a (BatchNormalizati (None, 7, 7, 512)    2048        res5b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 7, 7, 512)    0           bn5b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2b (Conv2D)         (None, 7, 7, 512)    2359808     activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2b (BatchNormalizati (None, 7, 7, 512)    2048        res5b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 7, 7, 512)    0           bn5b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2c (Conv2D)         (None, 7, 7, 2048)   1050624     activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2c (BatchNormalizati (None, 7, 7, 2048)   8192        res5b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_15 (Add)                    (None, 7, 7, 2048)   0           bn5b_branch2c[0][0]              \n",
      "                                                                 activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 7, 7, 2048)   0           add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2a (Conv2D)         (None, 7, 7, 512)    1049088     activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5c_branch2a (BatchNormalizati (None, 7, 7, 512)    2048        res5c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 7, 7, 512)    0           bn5c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2b (Conv2D)         (None, 7, 7, 512)    2359808     activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5c_branch2b (BatchNormalizati (None, 7, 7, 512)    2048        res5c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 7, 7, 512)    0           bn5c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2c (Conv2D)         (None, 7, 7, 2048)   1050624     activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5c_branch2c (BatchNormalizati (None, 7, 7, 2048)   8192        res5c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_16 (Add)                    (None, 7, 7, 2048)   0           bn5c_branch2c[0][0]              \n",
      "                                                                 activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 7, 7, 2048)   0           add_16[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "avg_pool (AveragePooling2D)     (None, 1, 1, 2048)   0           activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1, 1, 2048)   4196352     avg_pool[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1, 1, 14)     28686       dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 27,812,750\n",
      "Trainable params: 0\n",
      "Non-trainable params: 27,812,750\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "final_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = final_model.evaluate(X_test1, Y_test[:1548], verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.4777791537055673"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 216, 216, 3)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.expand_dims(X_test1[1244],axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 14)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test[1244].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[0.1340938 , 0.08060184, 0.06358925, 0.10319147, 0.04658259,\n",
       "          0.05314061, 0.06460004, 0.02945529, 0.10833786, 0.04490286,\n",
       "          0.08699969, 0.06189188, 0.0402847 , 0.08232816]]]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_model.predict(np.expand_dims(X_test1[1246],axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test[1246]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99123"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels[train_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99123"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "for i in train_index:\n",
    "    if i == 117552:\n",
    "        print (1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not using data augmentation.\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:96: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 56s 1ms/step - loss: 1.8310 - acc: 0.4033 - val_loss: 1.5878 - val_acc: 0.4911\n",
      "Epoch 2/200\n",
      "50000/50000 [==============================] - 46s 911us/step - loss: 1.4524 - acc: 0.5238 - val_loss: 1.4303 - val_acc: 0.5100\n",
      "Epoch 3/200\n",
      "50000/50000 [==============================] - 46s 912us/step - loss: 1.3211 - acc: 0.5594 - val_loss: 1.3327 - val_acc: 0.5523\n",
      "Epoch 4/200\n",
      "50000/50000 [==============================] - 46s 913us/step - loss: 1.2483 - acc: 0.5783 - val_loss: 1.2918 - val_acc: 0.5545\n",
      "Epoch 5/200\n",
      "50000/50000 [==============================] - 46s 915us/step - loss: 1.1996 - acc: 0.5946 - val_loss: 1.2544 - val_acc: 0.5705\n",
      "Epoch 6/200\n",
      "50000/50000 [==============================] - 46s 915us/step - loss: 1.1640 - acc: 0.6057 - val_loss: 1.2318 - val_acc: 0.5755\n",
      "Epoch 7/200\n",
      "50000/50000 [==============================] - 46s 914us/step - loss: 1.1364 - acc: 0.6135 - val_loss: 1.2085 - val_acc: 0.5819\n",
      "Epoch 8/200\n",
      "50000/50000 [==============================] - 46s 914us/step - loss: 1.1141 - acc: 0.6212 - val_loss: 1.1967 - val_acc: 0.5866\n",
      "Epoch 9/200\n",
      "50000/50000 [==============================] - 46s 914us/step - loss: 1.0951 - acc: 0.6267 - val_loss: 1.1843 - val_acc: 0.5900\n",
      "Epoch 10/200\n",
      "50000/50000 [==============================] - 46s 914us/step - loss: 1.0793 - acc: 0.6321 - val_loss: 1.1662 - val_acc: 0.5956\n",
      "Epoch 11/200\n",
      "50000/50000 [==============================] - 46s 914us/step - loss: 1.0654 - acc: 0.6371 - val_loss: 1.1520 - val_acc: 0.6009\n",
      "Epoch 12/200\n",
      "50000/50000 [==============================] - 46s 914us/step - loss: 1.0529 - acc: 0.6416 - val_loss: 1.1370 - val_acc: 0.6099\n",
      "Epoch 13/200\n",
      "49984/50000 [============================>.] - ETA: 0s - loss: 1.0422 - acc: 0.6434"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-138-0594a28295cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     94\u001b[0m               \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m               \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m               callbacks=[lr_reducer, early_stopper, csv_logger])\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Using real-time data augmentation.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1248\u001b[0m                             val_outs = self._test_loop(val_f, val_ins,\n\u001b[1;32m   1249\u001b[0m                                                        \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1250\u001b[0;31m                                                        verbose=0)\n\u001b[0m\u001b[1;32m   1251\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1252\u001b[0m                                 \u001b[0mval_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mval_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_test_loop\u001b[0;34m(self, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1424\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1426\u001b[0;31m                 \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1427\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1428\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2478\u001b[0m             \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2479\u001b[0m         \u001b[0mfetches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdates_op\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2480\u001b[0;31m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2481\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m   2482\u001b[0m                               **self.session_kwargs)\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_MANUAL_VAR_INIT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m             \u001b[0mvariables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m             \u001b[0mcandidate_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36mglobal_variables\u001b[0;34m(scope)\u001b[0m\n\u001b[1;32m   1411\u001b[0m     \u001b[0mA\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mobjects\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1412\u001b[0m   \"\"\"\n\u001b[0;32m-> 1413\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGLOBAL_VARIABLES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1414\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mget_collection\u001b[0;34m(key, scope)\u001b[0m\n\u001b[1;32m   5879\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mend_compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5880\u001b[0m   \"\"\"\n\u001b[0;32m-> 5881\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5882\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mget_collection\u001b[0;34m(self, name, scope)\u001b[0m\n\u001b[1;32m   3941\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3942\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mscope\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3943\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcollection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3944\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3945\u001b[0m         \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ReduceLROnPlateau, CSVLogger, EarlyStopping\n",
    "\n",
    "import numpy as np\n",
    "from keras.applications.vgg16 import VGG16\n",
    "\n",
    "\n",
    "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1), cooldown=0, patience=5, min_lr=0.5e-6)\n",
    "early_stopper = EarlyStopping(min_delta=0.001, patience=10)\n",
    "csv_logger = CSVLogger('resnet18_cifar10.csv')\n",
    "\n",
    "batch_size = 32\n",
    "nb_classes = 10\n",
    "nb_epoch = 200\n",
    "data_augmentation = False\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 32, 32\n",
    "# The CIFAR10 images are RGB.\n",
    "img_channels = 3\n",
    "\n",
    "# The data, shuffled and split between train and test sets:\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# subtract mean and normalize\n",
    "mean_image = np.mean(X_train, axis=0)\n",
    "X_train -= mean_image\n",
    "X_test -= mean_image\n",
    "X_train /= 128.\n",
    "X_test /= 128.\n",
    "\n",
    "X_train1 = np.ndarray(shape=(50000,48,48,3))\n",
    "for i in range(50000):\n",
    "    X_train1[i]=cv2.resize(X_train[i][:,:,:], dsize=(48, 48), interpolation=cv2.INTER_LANCZOS4)\n",
    "\n",
    "X_test1 = np.ndarray(shape=((10000, 48, 48, 3)))\n",
    "for i in range(10000):\n",
    "    X_test1[i]=cv2.resize(X_test[i][:,:,:3], dsize=(48, 48), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "X_train = X_train1\n",
    "X_test = X_test1\n",
    "\n",
    "    \n",
    "#model = VGG16(weights='imagenet', include_top=False, input_shape=(48,48,3))\n",
    "#for layer in model.layers:\n",
    "#    layer.trainable = False\n",
    "    \n",
    "#x = Dense(512, activation='relu')(model.output)\n",
    "#predictions = Dense(10, activation='softmax')(x)\n",
    "\n",
    "#model = Model(inputs=model.input, outputs=predictions)\n",
    "\n",
    "#model.compile(loss='categorical_crossentropy',\n",
    "#              optimizer='adam',\n",
    "#              metrics=['accuracy'])\n",
    "\n",
    "model_RGB = VGG16(weights='imagenet', include_top=False, input_shape=(48,48,3))\n",
    "model_NRR = VGG16(weights='imagenet', include_top=False, input_shape=(48,48,3))\n",
    "\n",
    "for layer in model_NRR.layers:\n",
    "    layer.name = layer.name + str(\"NRR\")\n",
    "\n",
    "for layer in model_RGB.layers:\n",
    "    layer.trainable = False\n",
    "for layer in model_NRR.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "combineFeatureLayer = keras.layers.concatenate([model_RGB.output, model_NRR.output])\n",
    "x = Dense(1024, activation='relu')(combineFeatureLayer)\n",
    "predictions = Dense(10, activation='softmax')(x)\n",
    "\n",
    "final_model = Model(inputs=[model_RGB.input,model_NRR.input], outputs=predictions)\n",
    "\n",
    "sgd = optimizers.SGD(lr=0.0001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "final_model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "\n",
    "if not data_augmentation:\n",
    "    print('Not using data augmentation.')\n",
    "    final_model.fit([X_train,X_train], np.expand_dims(np.expand_dims(Y_train,axis=1),axis=1),\n",
    "              batch_size=batch_size,\n",
    "              nb_epoch=nb_epoch,\n",
    "              validation_data=([X_test,X_test], np.expand_dims(np.expand_dims(Y_test,axis=1),axis=1)),\n",
    "              shuffle=True,\n",
    "              callbacks=[lr_reducer, early_stopper, csv_logger])\n",
    "else:\n",
    "    print('Using real-time data augmentation.')\n",
    "    # This will do preprocessing and realtime data augmentation:\n",
    "    datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=False)  # randomly flip images\n",
    "\n",
    "    # Compute quantities required for featurewise normalization\n",
    "    # (std, mean, and principal components if ZCA whitening is applied).\n",
    "    datagen.fit(X_train)\n",
    "\n",
    "    # Fit the model on the batches generated by datagen.flow().\n",
    "    model.fit_generator(datagen.flow(X_train, np.expand_dims(np.expand_dims(Y_train,axis=1),axis=1), batch_size=batch_size),\n",
    "                        steps_per_epoch=X_train.shape[0] // batch_size,\n",
    "                        validation_data=(X_test, np.expand_dims(np.expand_dims(Y_test,axis=1),axis=1)),\n",
    "                        epochs=nb_epoch, verbose=1, max_q_size=100,\n",
    "                        callbacks=[lr_reducer, early_stopper, csv_logger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
